:PROPERTIES:
:ID:       0cc6700c-1018-4309-8a5b-44359e171abe
:END:
#+TITLE: Hyperreal Enterprises: Roadmap
#+OPTIONS: H:3 num:t toc:nil ':t
#+LATEX_HEADER: \usepackage[a4paper,bindingoffset=0.2in,left=1in,right=1in,top=1in,bottom=1in,footskip=.25in]{geometry}
#+LATEX_HEADER: \usepackage[dvipsnames]{xcolor}
#+LATEX_HEADER: \usepackage{fontspec}
#+LATEX_HEADER: \usepackage[math-style=french]{unicode-math}
#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \setmathfont[math-style=upright]{DejaVu Sans Mono}
#+LATEX_HEADER: \setmonofont[scale=.8,Color=blue]{Ubuntu Mono}
#+LATEX_HEADER: \newfontfamily{\mm}[scale=.8,Color=red]{DejaVu Sans Mono}
#+LATEX_HEADER: \setmainfont[BoldFont=EB Garamond,BoldFeatures={Color=ff0000}]{EB Garamond}
#+LATEX_HEADER: \newcommand{\hookuparrow}{\mathrel{\rotatebox[origin=c]{90}{$\hookrightarrow$}}}
#+LATEX_HEADER: \usepackage{fix-abstract}
#+LATEX_HEADER: \definecolor{pale}{HTML}{fffff8}
#+LATEX_HEADER: \definecolor{orgone}{HTML}{83a598}
#+LATEX_HEADER: \definecolor{orgtwo}{HTML}{fabd2f}
#+LATEX_HEADER: \definecolor{orgthree}{HTML}{d3869b}
#+LATEX_HEADER: \definecolor{orgfour}{HTML}{fb4933}
#+LATEX_HEADER: \definecolor{orgfive}{HTML}{b8bb26}
#+LATEX_HEADER: \definecolor{gruvbg}{HTML}{1d2021}
#+LATEX_HEADER: \newenvironment*{emptyenv}{}{}
#+LATEX_HEADER: \usepackage{sectsty}
#+LATEX_HEADER: \sectionfont{\normalfont\color{red}\selectfont}
#+LATEX_HEADER: \subsectionfont{\normalfont\selectfont}
# #+LATEX_HEADER: \subsubsectionfont{\normalfont\selectfont}
#+LATEX_HEADER: \paragraphfont{\normalfont\selectfont}
#+LATEX_HEADER: \subsubsectionfont{\normalfont\selectfont\color{black!50}}

\begin{abstract}
\noindent This document can be thought of as an informal outline of a “\emph{tactic state}”.  On a technical level, the upstream source for this material is an Org Roam graph.  The “Wiki” section contains instructions for accessing the material and generating derived formats, such as the Org Agenda.
\end{abstract}

\setcounter{tocdepth}{2}
\tableofcontents
# IMPORT
* Hyperreal Enterprises: Roadmap
:PROPERTIES:
  :tag: HL AN
  :END:
#+CATEGORY: ROADMAP

** Preface
:PROPERTIES:
:ID:       0caba40b-2561-4143-b2b1-55f3ddc3201b
:END:

This document synthesises a [[http://www.peeragogy.org/pattern-roadmap.html][Roadmap]] (and perhaps also a [[http://scrumbook.org/value-stream/product-roadmap.html][Product
Roadmap]]) for Hyperreal Enterprises, Ltd.  The Roadmap is being written
inside [[https://github.com/org-roam/org-roam][Org Roam]] (an [[https://www.gnu.org/software/emacs/][Emacs]] package), and displayed with [[https://github.com/theiceshelf/firn][Firn]].  The
document can be thought of as a [[*Wiki][Wiki]] (and edited inside Emacs).  From
these sources, other downstream formats can be derived.  A narrative
introduction follows.

To skip it, browse ahead to [[*Top][Top]].

** Introduction

*** Applying AI to technical fields is a huge opportunity.

It’s striking that in computer programming work, /collective
intelligence/ is used almost everywhere, but artificial intelligence is
used almost nowhere.  AI can kick butt at Chess and Go, but university
courses are still taught by human professors.  Surely, AI that could
write code at a human level would be a valuable thing...

*** But where’s the human-level AI for coding?

A research agenda around “AI for programming” was already spelled out
by Alan Turing in the late 1940s.  It is only relatively recently that
we have massive amounts of relevant data to work with.  Code
completion and code generation tools are some of the “low hanging
fruit” of this domain.  We think that something much more substantial
is around the corner.

*** We are looking at this problem as an informal open source R&D collective.

Our loose-knit team brings together applied experience in Natural Language
Processing (NLP), computational modelling, online communities,
mathematical sciences ($x!$) as well as humanities and social science.  We’ve been getting
together for daily coffee chats, and sharing information and skills
with each other in online seminars.  Sometimes we bring in guest
speakers.  We hope as time goes by our chats will coalesce into notes,
blog posts, papers, and working prototypes.  We aren’t promising a
schedule of deliverables — because we’re all volunteers — and we’re
doing this for fun and interest.  But we do plan to share what we’re
learning as we go along!

*** Broadly, the steps we have in mind go from data, to models, to AI agents.

We plan to document our progress (or lack thereof) on this blog.  As a
very rough outline, this is what we expect to look at:

- We plan to use contemporary information extraction methods to derive computationally meaningful material from Stack Exchange Q&A, Github Issues, and programmers’ discussions, along with code. To do this, we will combine general purpose language models, like BERT, and a knowledge graph approach.

- We plan to use category theoretic methods as a glue that can hold together a range of computational models, including models of programs and the process of computer programming. [[https://arxiv.org/pdf/1807.05691][Monocl]] is an existing process modelling language that has been used to create an abstraction layer over a collection of [[https://www.datascienceontology.org/][data science programs]]. We plan to generalise this.

- Ultimately, we plan to install these models in computational agents who can then “converse with each other to sharpen their wits,” as Turing anticipated, mirroring contemporary developments in self-play.  However, the specific design of the agents remains an open issue at the moment!  We plan to explore techniques from Bayesian learning, logic programming, and reinforcement learning.

*** Reflection is part of the process.

We’re interested in understanding human behaviour as well as technical
topics: that goes for our own behaviour in particular.  We plan to
post to our blog at least monthly — as long as we keep our discussion
running — using it as a place to reflect on how things are going.
Although we’re grappling with some weighty topics, success is not all
or nothing!  Writing here will mark our progress and be useful in
their own right (e.g., blog posts can feed into research papers or
tools).

*** Progress so far
Alongside setting up a blog and drafting an initial anouncement
(voilà!), we’ve updated the website of our affiliated UK-based company
[[https://hyperreal.enterprises/][Hyperreal Enterprises]] to match the outlines of what we’ve described
here.  As mentioned above, are having regular meetings, which we’ll
record if they look likely to be interesting to a wider audience.
We’ve got nice internal documentation going, via [[https://github.com/org-roam/org-roam][Org Roam]], from wich
various derived formats are produced, including a public [[https://exp2exp.github.io/][wiki]]
generated by [[https://github.com/theiceshelf/firn][Firn]].  (We recently sent an abstract to EmacsConf 2020 to
talk about the various other Emacs-based tools we’re using!)

Some of our seminars so far have explored an Open Information
Extraction (with several demo notebooks), and looked at the early
parts of Michael Betancourt’s “Towards a principled Bayesian
workflow”.

Active channels on our Discord server include:
- #emacs-cloud-hypernotebooks
- #how-to-design-programs
- #text-analysis
- #model-construction
- #knowledge-graph

*** TL;DR

We are creating AI tools that will process open source information and
build on these in applications such as automated programming.

* Top
:PROPERTIES:
  :tag: HL
  :END:

** Motivation: For the sake of advancing AI
:PROPERTIES:
:ID:       744b12b2-b93c-4ad9-9fd1-5f649eac548f
:END:

We are doing this R&D work partly to make demonstrations of more
advanced AI systems.  We expect that our stance on AI will not
necessarily be a popular one.  But this is an important “minor” strand
of AI research dating back to Alan Turing:

#+begin_quote
"As time goes on the [computer] itself will take over the functions
both of [programmers] and of [users]…The [programmers] are liable to
get replaced because as soon as any technique becomes at all
stereotyped it becomes possible to devise a system of instruction
tables which will enable the electronic computer to do it for
itself. It may happen however that the [programmers] will refuse to do
this. They may be unwilling to let their jobs be stolen from them in
this way. In that case they would surround the whole of their work
with mystery and make excuses, couched in well chosen gibberish,
whenever any dangerous suggestions were made." -- Alan Turing, 1947.
#+end_quote

On average, advanced AI would bring in new ways of working, and would
facilitate broad access to high-quality training.  This agenda could
serve to focus the mind of technical workers, but not many are
pursuing it presently.

** Motivation: Technical experiments become easier

Even in the present time, without relying on any speculative AI
futures to magically appear, we can benefit from pursuing the agenda
above.  Accordingly, we are doing some applied work with existing
software that will give us a set of further tools and levers to work
with.

** Representative Prior Work

*** PlanetMath

[[https://planetmath.org/][PlanetMath]] users created a reasonably large informal mathematical
knowledge base together.  On the way, we came up with several
technical demos and sketched possible [[https://github.com/holtzermann17/planetmath-docs/labels/PREVIEW][previews]] for upcoming features.
One possible direction of work we looked at would be to focus on
building a comprehensive category theory knowledge base.

*** Modelling the way mathematics is actually done

In [[https://www.newton.ac.uk/files/preprints/ni17003.pdf][this paper]], we talked about how mathematics is situated somewhere
in between ‘games’ and ‘storytelling’ in its complexity.  We proposed
to build computational models of informal mathematical reasoning.
Subsequent work continued on in this direction, using ideas from
[[https://www.sciencedirect.com/science/article/pii/S0004370217300267][dialogue games]] and [[https://link.springer.com/article/10.1007/s10503-018-9474-x][argumentation theory more broadly]].

** A sketch of a plan

So, having gotten together around these ideas, we’re having online
chat, frequent short meetings.  We’ve talked about maintaining a blog
that would describe what we’re learning and developing.  So, roughly
speaking, we will try to develop a curriculum through the blog.  We
also have this wiki, that any of us can edit, which we can use as a
staging ground for more developed blog posts.  Our thought was that
blog posts might move in the direction of more developed outputs,
whether products or research papers.  We want to use some ideas
adapted from Scrum to build a shared awareness of what’s going on.
However, we want to be careful not to become “managerial” since
everyone is currently here as a volunteer, working on topics of his or
her own interest.  We want to provide mutual support and fun.
Reflection, whether in writing, or by recording and listening again to
conversations, should help with that.  We are not constraining things
to come out in a purely structured curriculum, or any other form of
product development.  “Users” and “customers” may appear as we release
things we are happy with and expand our little community.

#+begin_quote
“Rousseau says, someone who has been properly educated will be engaged
in society, but relate to his or her fellow citizens in a natural way.
... We naturally look after our own
preservation and interests.  By contrast, /amour-propre/ is an unnatural
self-love that is essentially relational. ... Thus, /amour-propre/ can
contribute positively to human freedom and even virtue. Nevertheless,
/amour-propre/ is also extremely dangerous because it is so easily
corruptible. ... In its corrupted form, /amour-propre/ is the source of
vice and misery, and results in human beings basing their own self
worth on their feeling of superiority over others.” — [[https://iep.utm.edu/rousseau/][IEP]]
#+end_quote

** A possible formulation: short correlated sprints as opposed to random behaviour

“Two people working together 4 hours a week for two weeks” could serve
as an approximate unit of work.  Once we have amassed a few outputs
from this kind of effort, we will have some evidence of the kinds of
things that we can realistically achieve.  So far, our workflow has
been more based on solo activities and informal conversations, but
short robust team-ups continue to be an option!

#+begin_quote
Hypothetical conversation: /In my next post I want to integrate something that I learned from you about PL.  I want to drive in the direction of synthesis, as hard as I know how to right now.  This depends on everyone having free time to invest in this.  Start a blog where we think about what's the overlap in terms of learning?/
#+end_quote


** Subgoals:                                                        :noexport:
:PROPERTIES:
:ID:       1d3660fd-8826-4afb-b1e4-91b20c27ee9a
:END:

- [[*Why not what][Why not what]]
- [[*Which model construction process works as a whole?][Which model construction process works as a whole?]]
- [[*Underlying foundation][Underlying foundation]]
- [[*Construct, critique, improve models of the creative process][Construct, critique, improve models of the creative process]]
* Why not what
:PROPERTIES:
  :tag: HL
  :END:

Our purpose:

- We want to make the knowledge economy accessible to everyone.
- Our long-term vision is computational intelligence based on collective intelligence.
** Teach arbitrary coding
:PROPERTIES:
  :tag: LRD
  :END:

This would be an abstraction over teaching basic programming and
knowledge graphs.

*** Feature: Production system

We’ve started to build a simple production system that can be used to
detect errors in subtraction (reimplementing some classic work). We
were thinking that something similar could be used to detect other
kinds of errors (so, for debugging, teaching), and to support other
kinds of reasoning processes (e.g., turning Q’s into A’s in a
question-answering system).

We previously did a little exploratory work, with similar intent, using
polygraphs as input, in the workshop paper
/Modelling the Way Mathematics Is Actually Done/.

**** Demo application: Reimplementing classic rules to model subtraction

We looked at a classic paper about “subtraction on Mars” and it seems
that reimplementing it might be the best way to go.

*** Contributes to                                                 :noexport:
:PROPERTIES:
:ID:       98bd73a0-035b-434c-aa2e-ea0e3e6ec15d
:END:
- [[*BUSINESS DEVELOPMENT][BUSINESS DEVELOPMENT]]

** How to Design Programs
:PROPERTIES:
  :tag: HD
  :END:

We were thinking of /How to Design Programs/ (HtDP) as a potential
basis for this work.  We would want to respect category theoretic
concepts in the presentation.  We would expect to find analogues in
settings like Bayesian modelling.

We could proceed by looking at relationships with argumentation
theory, thinking about how to do this in a theoretically consistent
way.  Once we have a definition of the programming language we’re
going to use, we can then do argumentation over that.

Another strategy would be to develop a DSL for HtDP ideas, which we
could then reuseq to generate patterns for learning how to design
various structures (say, web pages or probabilistic programs).  To do
this well you’d need ways to express ‘recipes’.  For example, an MVP
might be based on representing HtDP-style recipes using sequent
calculi for session types.  These represent interactive protocols.

You’d use cut-elimination to have two players interact (using
something like the **Lakatos Game diagram**).  But what formalism
would you use?  E.g., /geometry of interaction in linear logic/ has
been used for this kind of thing, but could it be used here?  With a
suitable formalism in place we would then imagine that a computer
programming agent would just follow the “Lakatos Game” style HtDP
script.  So, this would contribute to the development of agent models
for programming and program-related Q&A.

*** Related work

- General theory-informed algorithms (e.g., apply category theory to scientific models).
- K framework: Have transformations for any language you define in it.
- HtDP is similar applied to programming teaching.  Start with PL theory and then find universal things.
- How can we define statistics in a general way and then derive things from it?  (E.g., Anglican probabilistic programming?)

*** Contributes to                                                 :noexport:
:PROPERTIES:
:ID:       e5d35810-ca01-48f7-90f1-0681fa548385
:END:

- [[*Teach arbitrary coding][Teach arbitrary coding]]
- [[*Agent model][Agent model]]
* Construct, critique, improve models of the creative process
:PROPERTIES:
  :tag: HL
  :END:

We want tools and processes for working with models, with a particular
emphasis on improved models of the creative process. The reason for
this emphasis is that if we have good models of the creative process,
including the modelling process, we can then apply them to a wide
range of problems!  This prompts reflection on the infrastructure and
tools that we are actually using.

** Subgoals :noexport:
:PROPERTIES:
:ID:       0fea67e1-6088-4845-9eeb-c080609bf58d
:END:

- [[*Emacs Hyper Notebook][Emacs Hyper Notebook]]
- [[*How to Design Programs][How to Design Programs]]
- [[*Probabilistic programming for scientific modelling][Probabilistic programming for scientific modelling]]
- [[*Information extraction from SO Q&A items][Information extraction from SO Q&A items]]
- [[*Arxana][Arxana]]
** Emacs Hyper Notebook
:PROPERTIES:
  :tag: CDN TO
  :END:
#+CATEGORY: DEV

We are developing a better way to do “Jupyter notebooks” using Emacs.
This recovers some of the Research Collab ideas developed by Aaron
Krowne. It should integrate features such as writing and task
management (e.g., /Org/) Program evaluation (e.g., /Maxima/),
Typesetting and presentation (e.g., slides via /LaTeX/), and
navigation (e.g., /Org Roam/ for displaying topics as a graph).  We
should be clear that the various technologies used are slot-fillers
and they might be replaced with other things, or augmented (e.g.,
/Lean/ for formal verification of some of the above?). A useful input
to this process would be implementation of examples without
integration.  This can then be redone in a more integrated fashion.

An integration using existing technologies will have limitations, once
we have this demos then we will see some of the gaps and how more
advanced tech could be useful. (For example, Ray’s work with Gerschom
could turn out to be useful here.)

**** Some quick thoughts

- If it was sitting inside a web container, then maybe it’s a quickstart thing that comes in a user friendly form.
- Maybe add menu-bar items to make it look like a web browser.
- Emacs Maxima interface, we might build on it — for Monday 12th October, a quick "15 minutes" talk to catch up
- ... possible deliverable for later on: a NIST talk?

*** DONE [#A] Figure out subtasks to deliver for EmacsConf  :joe:ray:cameron:
*** BACK Figure out how EHN relates to other projects
*** DONE [#A] Keep testing crdt.el and lockstep.el                  :joe:ray:
*** BACK Could Emacsconf talk become a blog post?           :joe:ray:cameron:

(Notice that with crdt, typing can go on inside folded nodes! Qiantan
is thinking about a mode to make overlays shareable, which would
change things a bit.)

*** Partial prototypes

How far can we go... Through [[https://roamresearch.com/][Roam]]? (We could at least talk to Connor
about Roam on Twitter?) Through [[https://jupyter.org/][Jupyter]]? [[https://foambubble.github.io/foam/][Foam]]? [[https://gtoolkit.com/][Glamorous Toolkit]]?  Can
we integrate what we’re building with existing tools like these?  Do
Lenses or other kinds of ACT machinery help with this at all?  Would
our system potentially play a role as a universal backend?

*** Feature: Arxana 2020

Revisit [[https://repo.or.cz/w/arxana.git][Arxana]] and turn it into something that we can actually use.
This is rather closely related to the use of “knowledge graph”
formulations we’ve been discussing, since Arxana allows us to combine
writing with knowledge representations.  In our last round of work
with Arxana, we left off at the point of integrating logic programming
into the system.

*** Links to useful resources

Technology like this could be used to build simple demos (e.g., Emacs
in the browser, running Org Mode).  We’ve noticed some other related
tools as well, like [[https://github.com/200ok-ch/organice][Organice]] and [[https://github.com/tecosaur/codiorg][CodiOrg]] that could provide
alternative interfaces.

- [[https://github.com/exp2exp/notebooks][exp2exp/notebooks: This is a Docker configuration for running jupyter with multiple kernels on Arch Linux.]]
- [[https://www.gnu.org/software/emacs/manual/html_node/emacs/emacsclient-Options.html][emacsclient Options - GNU Emacs Manual]]
- [[https://github.com/butlerx/wetty][butlerx/wetty: Terminal in browser over http/https. (Ajaxterm/Anyterm alternative, but much better)]]
- [[https://github.com/xtermjs/xterm.js#real-world-uses][xtermjs/xterm.js: A terminal for the web]]
- [[https://twitter.com/cianbutlerx]]
- [[https://github.com/tsl0922/ttyd][tsl0922/ttyd: Share your terminal over the web]]
- [[https://github.com/yudai/gotty][yudai/gotty: Share your terminal as a web application]]
- [[https://hub.docker.com/r/butlerx/wetty][butlerx/wetty - Docker Hub]]
- [[https://medium.com/@pacroy/setup-web-terminal-using-wetty-docker-image-dcb1ea75bfaf][Setup Web Terminal using Wetty Docker Image | by Chairat Onyaem (Par) | Medium]]
- [[https://hub.docker.com/r/krishnasrinivas/wetty/][krishnasrinivas/wetty - Docker Hub]]

*** Other related work

- James Fairbanks (relate this to Betancourt).

*** Testing

#+begin_src clojure :session :backend cider :results output org
(def a 2)
#+end_src

#+RESULTS:
#+begin_src org
#'user/a
#+end_src

#+begin_src clojure :session :backend cider :results output org
a
#+end_src

#+RESULTS:
#+begin_src org
2
#+end_src

#+begin_src clojure :session :backend cider :results output org
(range 10)
(def a 1)
#+end_src

#+RESULTS:
#+begin_src org
| (0 1 2 3 4 5 6 7 8 9) |
| #'user/a              |
#+end_src

#+begin_src clojure :session :backend cider :results output org
a
#+end_src

#+RESULTS:
#+begin_src org
1
#+end_src

*** Implementing a quick demo for Emacs NYC
  :PROPERTIES:
  :ID:       b9838bdf-3b4a-4439-ad80-0c5e2d461b34
  :END:

Notes for the first talk will appear here:

- [[*Hypernotebook First Demo][Hypernotebook First Demo]]

*** What would we actually want as our org interface?

We had a short problem with this:

#+begin_src
1+1;
(error "No such language mode: nil-mode")
...
#+end_src

*** Backends
**** jupyter
:PROPERTIES:
:ID:       43fd0298-adec-400a-a9b6-6d48cfd244a6
:END:

The jupyter backend works well locally but suffers from a bug when run via tramp. See [[*emacs-jupyter remote debugging][emacs-jupyter remote debugging]]

**** ob-streams

This is work in progress, with some sample content above.

*** Future work

- Extending to VS Code?  Would people who use VS code even want this kind of interaction?  Maybe VS Code is better for quick visualisations?

*** Contributes to                                                 :noexport:
- [[*Visual Interfaces][Visual Interfaces]]
- [[*Knowledge graph][Knowledge graph]]
** Hypernotebook First Demo

First demo notes will go here.

*** During our first session on this, we set up a calculator

We wrote up a configuration, starting from what the =src= block might
look like:

- =:process= stands in for =:session= now as an alternative
- =org-babel-execute-src-block= this is the function that will be called
- We saw we had to hang into the =lang= parameter of the above function, but override using =:process=
- The variable =org-stream-output= needs to be defined (it's in our =ob-stream.el=).
- The function =org-babel-stream= needs to be defined as well (it's in our =ob-stream.el=).
- If you're going to use a language mode like =calc= it should be required.
- The process itself needs to be defined as well.
- =(setq org-confirm-babel-evaluate nil)= should be set.

This is an example of what we came up with:

=#+begin_src calc :stream calculator :results output org=

#+begin_src calc :stream calculator :results output org
10*8
#+end_src

#+RESULTS:
#+begin_src org
80
#+end_src

*** Background research

We took a good look at emacs-jupyter to understand how it works.
It seems to work reasonably well but not able to do all the hops via TRAMP to connect with a notebook running on Google Cloud.  This is documented as bug #191 in the emacs-jupyter repo.

*** Now we wanted to improve this to make it more robust

- Be able to handle multiple backends (now via "servants" for different shell commands and potentially other processes), not just the =bc= calculator!
- In the first version we just used a variable to store things, now use a hash table to organise the data better (=org-babel-servant-info=)
- Generalise everything

*** Stitching things together

- Find a good way to weave =org-babel-servant= into =org-babel-execute-src-block=
- Organise the callbacks, can we demo it calling Maxima?

**** Getting things going

#+begin_src emacs-lisp
;; Here we start the process
(get-buffer-create "maxima-error")
(setq maxima-proc
      (make-process
       :name "maxima-proc"
       :command '("maxima" "--very-quiet") ;; if we need parameters can add here
       :stderr "maxima-error"
       :filter #'org-stream-string-callback))

(setq calculator-proc
      (make-process
       :name "calculator-proc"
       :command '("bc" "-q") ;; if we need parameters can add here
       ;; :stderr "maxima-error"
       :filter #'org-stream-number-callback))
#+end_src

***** Calculator example again
#+begin_src calc :stream calculator-proc :results output org
10+1
#+end_src

#+RESULTS:
#+begin_src org
10
#+end_src

***** Maxima example

Here we call the process we just started.

#+begin_src exp :servant maxima-proc :results verbatim org
3+600000000;
#+end_src

#+RESULTS:
#+begin_src org
600000003
#+end_src

#+begin_src exp :servant maxima-proc :results output org
display2d:false;
#+end_src

#+RESULTS:
#+begin_src org
false
#+end_src

#+begin_src exp :servant maxima-proc :results output org
expand((x+1)^9)
#+end_src

#+RESULTS:
#+begin_src org
$$x^9+9\,x^8+36\,x^7+84\,x^6+126\,x^5+126\,x^4+84\,x^3+36\,x^2+9\,x+1$$
#+end_src

#+begin_src exp :servant maxima-proc :results output org
expand((x+1)^1)
#+end_src

#+RESULTS:
#+begin_src org
$$x+1$$
#+end_src

*** Ob-servant example

**** Calculator example again
#+begin_src calc :servant calculator :results output org
10+1
#+end_src

#+RESULTS:
#+begin_src org
11
#+end_src

*** Next steps

- Change the formatting of the output so it doesn't come across as a table
- Carry on with ob-servant to integrate it (improve calling Maxima, errors)
- Raise the change with the Org maintainers
- Package it up! (patch + repo?)
- 5 minute talk for Emacs New York (Monday 2 November 2020).

*** Social Networking

- Talk with Fermin to understand what he's doing with Maxima and see if we need to do anything differently

*** Final polishing

- Consider renaming it to ob-servent!
** emacs-jupyter remote debugging
:PROPERTIES:
  :tag: CDN
  :END:

*** Debugging

**** Initial fix (of what turned out to be a minor problem)
- =jupyter-start-kernel= was problematic, because the id didn’t seem to be set correctly
 - line -2 from end of this function, added =:id (substring conn-file -41 -5)=.

**** Ongoing concerns: can we access the kernel?
- However, this still doesn’t solve our problem
 - Now we are debugging =jupyter-kernel-info=
 - Our aim is to figure out =jupyter-send-kernel-info-request=, maybe also =jupyter-wait-until-received=

- jupyter-comm-initialize 
- jupyter-kernel-info
- jupyter-make-client
- jupyter-session — 
- jupyter-kernel-info — doesn’t get a message back for some reason

We suspect it looks at the wrong kernel b/c it examines an id that
doesn’t seem to exist on the server.

#+begin_src elisp
(with-slots (kernel) manager
  (oref kernel session))
#+end_src

*** Set up container on gcp
**** gcp configuration

You may like to run =gcloud auth login= ( [[https://cloud.google.com/sdk/gcloud/reference/auth/login][auth login docs]] ). This is an interactive process that launches oauth for your google account in the web browser so I think it is best to do it from a terminal though it may be possible to run it in org-babel.

#+BEGIN_SRC sh :session gcpsetup :results output verbatim replace :exports both
gcloud config configurations list
#+END_SRC

#+RESULTS:
: 
: NAME     IS_ACTIVE  ACCOUNT                  PROJECT  COMPUTE_DEFAULT_ZONE  COMPUTE_DEFAULT_REGION
: default  True       holtzermann17@gmail.com  quarere

#+BEGIN_SRC sh :session gcpsetup :results output verbatim replace :exports both
gcloud config configurations describe quarere
#+END_SRC

#+RESULTS:
: [1;31mERROR:[0m (gcloud.config.configurations.describe) The configuration [quarere] does not exist.

**** launch container image

Deploy a vm based on the container =cameronraysmith/notebooks:latest=.

#+BEGIN_SRC sh :session gcpsetup :results output verbatim replace :exports code
gcloud compute instances create-with-container notebooks-vm \
    --container-image registry.hub.docker.com/cameronraysmith/notebooks:latest \
    --container-restart-policy on-failure \
    --container-privileged \
    --container-stdin \
    --container-tty \
    --container-mount-host-path mount-path=/home/jupyter,host-path=/tmp,mode=rw \
    --machine-type n1-standard-1 \
    --boot-disk-size 50GB \
    --preemptible
#+END_SRC

Setup ssh with your new instance

#+BEGIN_SRC sh :session gcpsetup :results output verbatim replace :exports code
gcloud compute config-ssh
cat ~/.ssh/config | grep "Host notebooks"
#+END_SRC

#+RESULTS:
: You should now be able to use ssh/scp with your instances.
: For example, try running:
: ssh notebooks-vm.us-central1-f.quarere
: Host notebooks-vm.us-central1-f.quarere

You can =ssh= into the host machine or the container using the various commands below.

#+BEGIN_EXAMPLE sh
gcloud compute ssh notebooks-vm # into host machine
ssh notebooks-vm.us-central1-f.quarere docker ps -aqf "name=klt-notebooks-vm-cjme" # check the container ID
gcloud compute ssh notebooks-vm --container klt-notebooks-vm-cjme # use gcloud ssh with --dry-run to print the command
ssh -t notebooks-vm.us-central1-f.quarere -- sudo docker exec -it klt-notebooks-vm-cjme /bin/sh # this takes you directly into the container
#+END_EXAMPLE

Of course you can stop and start the machine with

#+BEGIN_EXAMPLE sh
gcloud compute instances stop notebooks-vm
gcloud compute instances start notebooks-vm
#+END_EXAMPLE

*** Startup the cloud vm running our container of interest
**** Setup remote container host machine

We already setup the container named =notebooks-vm= so all we need to do to begin with is to start it up.

#+BEGIN_SRC sh :results output verbatim replace :exports both :async yes
gcloud compute instances start notebooks-vm
#+END_SRC

#+RESULTS:

Check that our instance is indeed running

#+BEGIN_SRC sh :results output verbatim replace :exports both
gcloud compute instances list
#+END_SRC

#+RESULTS:
: NAME          ZONE           MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP     STATUS
: notebooks     us-central1-c  n1-standard-1               10.128.0.22                  TERMINATED
: notebooks-vm  us-central1-f  n1-standard-1  true         10.128.0.26  104.154.99.181  RUNNING

Make sure the correct ip address is entered into our =.ssh/config= file.
#+BEGIN_SRC sh :results output verbatim replace :exports both
gcloud compute config-ssh
#+END_SRC

#+RESULTS:
: You should now be able to use ssh/scp with your instances.
: For example, try running:
: 
:   $ ssh notebooks-vm.us-central1-f.quarere
: 

Inspect the IP address we find in our =.ssh/config= file

#+BEGIN_SRC sh :results output verbatim replace :exports both
grep HostName ~/.ssh/config
#+END_SRC

#+RESULTS:
:     HostName 104.154.99.181

***** Execute commands on the remote container host machine
#+BEGIN_SRC sh :session notebookshost :results output verbatim replace :exports both :dir /ssh:notebooks-vm.us-central1-f.quarere:
hostname --long
#+END_SRC

#+RESULTS:
:
: $ notebooks-vm.us-central1-f.c.quarere.internal

#+BEGIN_SRC sh :session notebookshost :results output verbatim replace :exports both :dir /ssh:notebooks-vm.us-central1-f.quarere:
docker container ls
#+END_SRC

#+RESULTS:
: 
: $ CONTAINER ID        IMAGE                                                                COMMAND                  CREATED             STATUS              PORTS               NAMES
: 4f31864fade2        registry.hub.docker.com/cameronraysmith/notebooks:latest             "/bin/sh -c 'jupyter…"   23 minutes ago      Up 23 minutes                           klt-notebooks-vm-cjme
: 19a4598c7503        gcr.io/stackdriver-agents/stackdriver-logging-agent:0.2-1.5.33-1-1   "/entrypoint.sh /usr…"   24 minutes ago      Up 24 minutes                           stackdriver-logging-agent

#+BEGIN_SRC sh :session notebookshost :results output verbatim replace :exports both :dir /ssh:notebooks.us-central1-c.quarere:
docker container ls
#+END_SRC

#+RESULTS:
:
: $ CONTAINER ID        IMAGE                              COMMAND                  CREATED             STATUS              PORTS                      NAMES
: caadc9a126bb        gcr.io/inverting-proxy/agent       "/bin/sh -c '/opt/bi…"   2 hours ago         Up 2 hours                                     proxy-agent
: 8080/tcp   payload-container

*** Run shell commands on the remote container
:PROPERTIES:
:header-args: :results output verbatim replace :session notebookscontainer :dir /ssh:notebooks-vm.us-central1-f.quarere|docker:klt-notebooks-vm-cjme:  :exports both  :eval never-export
:END:

To switch between two available configurations, choose one of the lines below to copy to the =:PROPERTIES:= drawer for this section.
#+BEGIN_EXAMPLE lisp
:header-args: :results output verbatim replace :session notebookscontainer :dir /ssh:notebooks-vm.us-central1-f.quarere|docker:klt-notebooks-vm-cjme:  :exports both  :eval never-export
:header-args: :results output verbatim replace :session notebookscontainer :dir /ssh:notebooks.us-central1-c.quarere|docker:payload-container:  :exports both  :eval never-export
#+END_EXAMPLE

In order to connect to the remote host followed by the docker container we specify the directory as =ssh:notebooks-vm= (including the extra details we got from =gcloud compute ssh-config=) followed by =docker:containername= where we got the container name from running =docker container ls= on the remote machine.

#+BEGIN_SRC sh
echo $JUPYTER_PATH
#+END_SRC

#+RESULTS:
: 
: $

#+BEGIN_SRC sh
head -3 /proc/self/cgroup
#+END_SRC

#+RESULTS:
: 12:cpuset:/docker/4f31864fade2d620150f5bdb8f162bfefd6528670dbe3769b7773570931445ff
: 11:freezer:/docker/4f31864fade2d620150f5bdb8f162bfefd6528670dbe3769b7773570931445ff
: 10:cpu,cpuacct:/docker/4f31864fade2d620150f5bdb8f162bfefd6528670dbe3769b7773570931445ff

Check the working directory and the list of jupyter kernels

#+BEGIN_EXAMPLE lisp
(push "-e" docker-tramp-docker-options)
(push "-e" "JUPYTER_PATH=/home/jovyan/.local/share/jupyter:/usr/local/share/jupyter:/usr/share/jupyter" docker-tramp-docker-options)
#+END_EXAMPLE

#+BEGIN_SRC sh
echo $JUPYTER_PATH
#+END_SRC

#+RESULTS:

#+BEGIN_SRC sh
jupyter kernelspec list
#+END_SRC

#+RESULTS:
: Available kernels:
:   ir           /home/jovyan/.local/share/jupyter/kernels/ir
:   julia-1.5    /home/jovyan/.local/share/jupyter/kernels/julia-1.5
:   maxima       /home/jovyan/.local/share/jupyter/kernels/maxima
:   python3      /usr/share/jupyter/kernels/python3

If you try to make use of an existing session on the docker container to run one of the =emacs-jupyter= kernels, you find that there is a different usage of the TRAMP remote path specification in the =:dir= property for the =sh= language of babel and with the =:session= property in the =emacs-jupyter= /language/ of babel. This is the error I got the first time I tried this with the TRAMP remote path specification in =:dir=:

#+BEGIN_EXAMPLE python
: FileNotFoundErrorTraceback (most recent call last)
: <ipython-input-1-d4b8d99aef95> in <module>
:       1 import os
:       2 __JUPY_saved_dir = os.getcwd()
: ----> 3 os.chdir("/ssh:notebooks-vm.us-central1-f.quarere|docker:klt-notebooks-vm-cjme:/home/jovyan/")
:       4 try:
:       5     get_ipython().run_cell("""x = 'foo'
:
: FileNotFoundError: [Errno 2] No such file or directory: '/ssh:notebooks-vm.us-central1-f.quarere|docker:klt-notebooks-vm-cjme:/home/jovyan/'
#+END_EXAMPLE

**** Run python session on the remote container
:PROPERTIES:
:header-args: :results output verbatim replace :session notebookscontainer-python :dir /ssh:notebooks-vm.us-central1-f.quarere|docker:klt-notebooks-vm-cjme:  :exports both  :eval never-export
:END:

The default properties that should apply to this section are

#+BEGIN_EXAMPLE elisp
:header-args: :results output verbatim replace :session notebookscontainer-python :dir /ssh:notebooks-vm.us-central1-f.quarere|docker:klt-notebooks-vm-cjme:/home/jovyan/  :exports both  :eval never-export
#+END_EXAMPLE

In order to connect to the remote host followed by the docker container we specify the directory as =ssh:notebooks-vm= (including the extra details we got from =gcloud compute ssh-config=) followed by a =|= and then =docker:containername= where we got the container name from running =docker container ls= on the remote machine.

#+BEGIN_SRC python
x = 'foo'
y = 'bar'
print(x + ' ' + y)
#+END_SRC

#+RESULTS:
: foo bar

#+BEGIN_SRC python
x = 1 + 1
print(x)
#+END_SRC

#+RESULTS:
: 2

*** BUG: Run a jupyter kernel in a remote container                :noexport:
:PROPERTIES:
:header-args: :results output verbatim replace :session /ssh:notebooks-vm.us-central1-f.quarere|docker:klt-notebooks-vm-cjme:notebooks01  :exports both  :eval never-export
:END:

**** properties
To switch between two available configurations, choose one of the lines below to copy to the =:PROPERTIES:= drawer for this section.
#+BEGIN_EXAMPLE lisp
:header-args: :results output verbatim replace :session /ssh:notebooks-vm.us-central1-f.quarere|docker:klt-notebooks-vm-cjme:notebooks01  :exports both  :eval never-export
:header-args: :results output verbatim replace :session /ssh:notebooks.us-central1-c.quarere|docker:payload-container:notebooks01  :exports both  :eval never-export
#+END_EXAMPLE

**** test code

#+BEGIN_SRC jupyter-python :kernel python3
x = 'foo'
y = 'bar'
x + ' ' + y
#+END_SRC

There is a problem whereby the =:dir= property is being passed along to jupyter as if it were a file. It looks like the intention may be to pass the TRAMP parameters to =:session= rather than =:dir= in the case of =emacs-jupyter=.

Here there is a bug that has been reported in [[https://github.com/nnicandro/emacs-jupyter/issues/191][issue 191 of emacs-jupyter]].

#+BEGIN_EXAMPLE lisp
executing Jupyter-Python code block...
jupyter-start-kernel: default-directory = /ssh:cloudmachine|docker:containeroncloudmachine:
jupyter-start-kernel: Starting process with args "/bin/python3 -c from jupyter_client.kernelapp import main; main() --kernel=python3"
Tramp: Opening connection for containeroncloudmachine using docker...
Tramp: Sending command ‘exec ssh -q    -e none cloudmachine’
Tramp: Waiting for prompts from remote shell...done
Tramp: Found remote shell prompt on ‘cloudmachine’
Tramp: Sending command ‘exec docker  exec -it  containeroncloudmachine sh’
Tramp: Waiting for prompts from remote shell...done
Tramp: Found remote shell prompt on ‘containeroncloudmachine’
Tramp: Opening connection for containeroncloudmachine using docker...done
Launching python3 kernel process...done
Tramp: Inserting ‘/ssh:cloudmachine|docker:containeroncloudmachine:/home/jovyan/.local/share/jupyter/runtime/kernel-fc5b0ea7-f553-4725-aa59-32829d356665.json’...
Tramp: Encoding remote file ‘/ssh:cloudmachine|docker:containeroncloudmachine:/home/jovyan/.local/share/jupyter/runtime/kernel-fc5b0ea7-f553-4725-aa59-32829d356665.json’ with ‘base64 <%s’...done
Tramp: Decoding local file ‘/var/folders/1d/wtzfcz5s4x98nbkdx9g5ss3c0000gn/T/tramp.krOJmR.json’ with ‘base64-decode-region’...done
Tramp: Inserting ‘/ssh:cloudmachine|docker:containeroncloudmachine:/home/jovyan/.local/share/jupyter/runtime/kernel-fc5b0ea7-f553-4725-aa59-32829d356665.json’...done
SENDING: :kernel-info-request ae928b51-f755-441e-a250-8a08c58d734d nil
SENT: (:shell ae928b51-f755-441e-a250-8a08c58d734d)
Requesting kernel info...done
jupyter-kernel-info: Kernel did not respond to kernel-info request
#+END_EXAMPLE

There is a [[https://github.com/nnicandro/emacs-jupyter/issues/72#issuecomment-543952258][suggestion from arthurcgusmao]] in another issue stating one needs to set the =JUPYTER_PATH= environment variable to resolve the =Kernel did not respond to kernel-info request= issue.

It is simple to set the =JUPYTER_PATH= environment variable via tramp

#+BEGIN_EXAMPLE elisp
(add-to-list 'tramp-remote-process-environment "JUPYTER_PATH=/home/jovyan/.local/share/jupyter:/usr/local/share/jupyter:/usr/share/jupyter")
#+END_EXAMPLE

however, this does not resolve the issue.

I originally tried to set the environment variable by passing a parameter to docker, but this did not work properly in the sense that if you check the value from inside the container it does not appear to be set despite what appears to be the appropriate docker flag for doing so.

#+BEGIN_EXAMPLE elisp
(push "-e" "JUPYTER_PATH=/home/jovyan/.local/share/jupyter:/usr/local/share/jupyter:/usr/share/jupyter" docker-tramp-docker-options)
(setq docker-tramp-docker-options
      '("-e" "JUPYTER_PATH=/home/jovyan/.local/share/jupyter:/usr/local/share/jupyter:/usr/share/jupyter"))
#+END_EXAMPLE

**** Debugging =jupyter-kernel-info=

=jupyter-kernel-info= is the function from which the error ~Kernel did not respond to kernel-info request~ arises (see [[https://github.com/nnicandro/emacs-jupyter/blob/403c70c83cb3754c83da0932b0efaf5e72bdca9a/jupyter-client.el#L2066][line 2066 of jupyter-client.el]]).

The stack trace for =jupyter-kernel-info=

#+BEGIN_EXAMPLE elisp
Debugger entered--entering a function:
jupyter-kernel-info(#<jupyter-org-client jupyter-org-client-1fe73d7aa114>)
jupyter--error-if-no-kernel-info(#<jupyter-org-client jupyter-org-client-1fe73d7aa114>)
jupyter-start-new-kernel("julia-1.5" jupyter-org-client)
jupyter-run-repl("julia-1.5" nil nil jupyter-org-client)
#f(compiled-function (session kernel) "Initiate a client connected to a remote kernel process." #<bytecode 0x1fe7435018f5>)(#s(org-babel-jupyter-remote-session :name "/ssh:notebooks-vm.us-central1-f.quarere|docker:klt..." :connect-repl-p nil) "julia-1.5")
apply(#f(compiled-function (session kernel) "Initiate a client connected to a remote kernel process." #<bytecode 0x1fe7435018f5>) (#s(org-babel-jupyter-remote-session :name "/ssh:notebooks-vm.us-central1-f.quarere|docker:klt..." :connect-repl-p nil) "julia-1.5"))
#f(compiled-function (&rest args) #<bytecode 0x1fe743520a15>)(#s(org-babel-jupyter-remote-session :name "/ssh:notebooks-vm.us-central1-f.quarere|docker:klt..." :connect-repl-p nil) "julia-1.5")
apply(#f(compiled-function (&rest args) #<bytecode 0x1fe743520a15>) (#s(org-babel-jupyter-remote-session :name "/ssh:notebooks-vm.us-central1-f.quarere|docker:klt..." :connect-repl-p nil) "julia-1.5"))
#f(compiled-function (&rest cnm-args) #<bytecode 0x1fe7430d03fd>)()
#f(compiled-function (cl--cnm session kernel) "Rename the returned client's REPL buffer to include SESSION's name.\nAlso set `jupyter-include-other-output' to nil for the session so\nthat output produced by other clients do not get handled by the\nclient." #<bytecode 0x1fe7434f577d>)(#f(compiled-function (&rest cnm-args) #<bytecode 0x1fe7430d03fd>) #s(org-babel-jupyter-remote-session :name "/ssh:notebooks-vm.us-central1-f.quarere|docker:klt..." :connect-repl-p nil) "julia-1.5")
apply(#f(compiled-function (cl--cnm session kernel) "Rename the returned client's REPL buffer to include SESSION's name.\nAlso set `jupyter-include-other-output' to nil for the session so\nthat output produced by other clients do not get handled by the\nclient." #<bytecode 0x1fe7434f577d>) #f(compiled-function (&rest cnm-args) #<bytecode 0x1fe7430d03fd>) (#s(org-babel-jupyter-remote-session :name "/ssh:notebooks-vm.us-central1-f.quarere|docker:klt..." :connect-repl-p nil) "julia-1.5"))
#f(compiled-function (&rest args) #<bytecode 0x1fe743520a41>)(#s(org-babel-jupyter-remote-session :name "/ssh:notebooks-vm.us-central1-f.quarere|docker:klt..." :connect-repl-p nil) "julia-1.5")
apply(#f(compiled-function (&rest args) #<bytecode 0x1fe743520a41>) #s(org-babel-jupyter-remote-session :name "/ssh:notebooks-vm.us-central1-f.quarere|docker:klt..." :connect-repl-p nil) "julia-1.5")
org-babel-jupyter-initiate-client(#s(org-babel-jupyter-remote-session :name "/ssh:notebooks-vm.us-central1-f.quarere|docker:klt..." :connect-repl-p nil) "julia-1.5")
org-babel-jupyter-initiate-session-by-key("/ssh:notebooks-vm.us-central1-f.quarere|docker:klt..." ((:colname-names) (:rowname-names) (:result-params "replace") (:result-type . value) (:results . "replace") (:exports . "both") (:cache . "no") (:noweb . "no") (:hlines . "no") (:tangle . "no") (:eval . "never-export") (:async . "no") (:session . "/ssh:notebooks-vm.us-central1-f.quarere|docker:klt...") (:kernel . "julia-1.5")))
#f(compiled-function (&optional session params) "Initialize a Jupyter SESSION according to PARAMS." #<bytecode 0x1fe7439bd0c1>)("/ssh:notebooks-vm.us-central1-f.quarere|docker:klt..." ((:colname-names) (:rowname-names) (:result-params "replace") (:result-type . value) (:results . "replace") (:exports . "both") (:cache . "no") (:noweb . "no") (:hlines . "no") (:tangle . "no") (:eval . "never-export") (:async . "no") (:session . "/ssh:notebooks-vm.us-central1-f.quarere|docker:klt...") (:kernel . "julia-1.5")))
apply(#f(compiled-function (&optional session params) "Initialize a Jupyter SESSION according to PARAMS." #<bytecode 0x1fe7439bd0c1>) ("/ssh:notebooks-vm.us-central1-f.quarere|docker:klt..." ((:colname-names) (:rowname-names) (:result-params "replace") (:result-type . value) (:results . "replace") (:exports . "both") (:cache . "no") (:noweb . "no") (:hlines . "no") (:tangle . "no") (:eval . "never-export") (:async . "no") (:session . "/ssh:notebooks-vm.us-central1-f.quarere|docker:klt...") (:kernel . "julia-1.5"))))
org-babel-jupyter-initiate-session("/ssh:notebooks-vm.us-central1-f.quarere|docker:klt..." ((:colname-names) (:rowname-names) (:result-params "replace") (:result-type . value) (:results . "replace") (:exports . "both") (:cache . "no") (:noweb . "no") (:hlines . "no") (:tangle . "no") (:eval . "never-export") (:async . "no") (:session . "/ssh:notebooks-vm.us-central1-f.quarere|docker:klt...") (:kernel . "julia-1.5")))
org-babel-execute:jupyter-julia("x = \"foo\"\ny = \"bar\"\nprintln(x)\nprintln(y)" ((:colname-names) (:rowname-names) (:result-params "replace") (:result-type . value) (:results . "replace") (:exports . "both") (:cache . "no") (:noweb . "no") (:hlines . "no") (:tangle . "no") (:eval . "never-export") (:async . "no") (:session . "/ssh:notebooks-vm.us-central1-f.quarere|docker:klt...") (:kernel . "julia-1.5")))
#f(compiled-function (&optional arg info params) "Execute the current source code block.\nInsert the results of execution into the buffer.  Source code\nexecution and the collection and formatting of results can be\ncontrolled through a variety of header arguments.\n\nWith prefix argument ARG, force re-execution even if an existing\nresult cached in the buffer would otherwise have been returned.\n\nOptionally supply a value for INFO in the form returned by\n`org-babel-get-src-block-info'.\n\nOptionally supply a value for PARAMS which will be merged with\nthe header arguments specified at the front of the source code\nblock." (interactive nil) #<bytecode 0x1fe742a390dd>)(nil nil nil)
ob-async-org-babel-execute-src-block(#f(compiled-function (&optional arg info params) "Execute the current source code block.\nInsert the results of execution into the buffer.  Source code\nexecution and the collection and formatting of results can be\ncontrolled through a variety of header arguments.\n\nWith prefix argument ARG, force re-execution even if an existing\nresult cached in the buffer would otherwise have been returned.\n\nOptionally supply a value for INFO in the form returned by\n`org-babel-get-src-block-info'.\n\nOptionally supply a value for PARAMS which will be merged with\nthe header arguments specified at the front of the source code\nblock." (interactive nil) #<bytecode 0x1fe742a390dd>) nil)
apply(ob-async-org-babel-execute-src-block #f(compiled-function (&optional arg info params) "Execute the current source code block.\nInsert the results of execution into the buffer.  Source code\nexecution and the collection and formatting of results can be\ncontrolled through a variety of header arguments.\n\nWith prefix argument ARG, force re-execution even if an existing\nresult cached in the buffer would otherwise have been returned.\n\nOptionally supply a value for INFO in the form returned by\n`org-babel-get-src-block-info'.\n\nOptionally supply a value for PARAMS which will be merged with\nthe header arguments specified at the front of the source code\nblock." (interactive nil) #<bytecode 0x1fe742a390dd>) nil)
org-babel-execute-src-block(nil)
(cond ((eq type 'headline) (cond ((memq (and (boundp 'org-goto-map) org-goto-map) (current-active-maps)) (org-goto-ret)) ((and (fboundp 'toc-org-insert-toc) (member "TOC" (org-get-tags))) (toc-org-insert-toc) (message "Updating table of contents")) ((string= "ARCHIVE" (car-safe (org-get-tags))) (org-force-cycle-archived)) ((or (org-element-property :todo-type context) (org-element-property :scheduled context)) (org-todo (if (eq (org-element-property :todo-type context) 'done) (or (car ...) 'todo) 'done)))) (org-update-checkbox-count) (org-update-parent-todo-statistics) (if (and (fboundp 'toc-org-insert-toc) (member "TOC" (org-get-tags))) (progn (toc-org-insert-toc) (message "Updating table of contents"))) (let* ((beg (if (org-before-first-heading-p) (line-beginning-position) (save-excursion (org-back-to-heading) (point)))) (end (if (org-before-first-heading-p) (line-end-position) (save-excursion (org-end-of-subtree) (point)))) (overlays (condition-case nil (progn (overlays-in beg end)) (error nil))) (latex-overlays (cl-find-if #'(lambda ... ...) overlays)) (image-overlays (cl-find-if #'(lambda ... ...) overlays))) (+org--toggle-inline-images-in-subtree beg end) (if (or image-overlays latex-overlays) (org-clear-latex-preview beg end) (org--latex-preview-region beg end)))) ((eq type 'clock) (org-clock-update-time-maybe)) ((eq type 'footnote-reference) (org-footnote-goto-definition (org-element-property :label context))) ((eq type 'footnote-definition) (org-footnote-goto-previous-reference (org-element-property :label context))) ((memq type '(timestamp planning)) (org-follow-timestamp-link)) ((memq type '(table-row table)) (if (org-at-TBLFM-p) (org-table-calc-current-TBLFM) (condition-case nil (progn (save-excursion (goto-char (org-element-property :contents-begin context)) (org-call-with-arg 'org-table-recalculate (or arg t)))) (error nil)))) ((eq type 'table-cell) (org-table-blank-field) (org-table-recalculate arg) (if (and (string-empty-p (string-trim (org-table-get-field))) (and (boundp 'evil-local-mode) evil-local-mode)) (progn (evil-change-state 'insert)))) ((eq type 'babel-call) (org-babel-lob-execute-maybe)) ((eq type 'statistics-cookie) (save-excursion (org-update-statistics-cookies arg))) ((memq type '(inline-src-block src-block)) (org-babel-execute-src-block arg)) ((memq type '(latex-environment latex-fragment)) (org-latex-preview arg)) ((eq type 'link) (let* ((lineage (org-element-lineage context '(link) t)) (path (org-element-property :path lineage))) (if (or (equal (org-element-property :type lineage) "img") (and path (image-type-from-file-name path))) (+org--toggle-inline-images-in-subtree (org-element-property :begin lineage) (org-element-property :end lineage)) (org-open-at-point arg)))) ((org-element-property :checkbox (org-element-lineage context '(item) t)) (let ((match (and (org-at-item-checkbox-p) (match-string 1)))) (org-toggle-checkbox (if (equal match "[ ]") '(16))))) (t (if (or (org-in-regexp org-ts-regexp-both nil t) (org-in-regexp org-tsr-regexp-both nil t) (org-in-regexp org-link-any-re nil t)) (call-interactively #'org-open-at-point) (+org--toggle-inline-images-in-subtree (org-element-property :begin context) (org-element-property :end context)))))
(let* ((context (org-element-context)) (type (org-element-type context))) (while (and context (memq type '(verbatim code bold italic underline strike-through subscript superscript))) (setq context (org-element-property :parent context) type (org-element-type context))) (cond ((eq type 'headline) (cond ((memq (and (boundp ...) org-goto-map) (current-active-maps)) (org-goto-ret)) ((and (fboundp 'toc-org-insert-toc) (member "TOC" (org-get-tags))) (toc-org-insert-toc) (message "Updating table of contents")) ((string= "ARCHIVE" (car-safe (org-get-tags))) (org-force-cycle-archived)) ((or (org-element-property :todo-type context) (org-element-property :scheduled context)) (org-todo (if (eq ... ...) (or ... ...) 'done)))) (org-update-checkbox-count) (org-update-parent-todo-statistics) (if (and (fboundp 'toc-org-insert-toc) (member "TOC" (org-get-tags))) (progn (toc-org-insert-toc) (message "Updating table of contents"))) (let* ((beg (if (org-before-first-heading-p) (line-beginning-position) (save-excursion ... ...))) (end (if (org-before-first-heading-p) (line-end-position) (save-excursion ... ...))) (overlays (condition-case nil (progn ...) (error nil))) (latex-overlays (cl-find-if #'... overlays)) (image-overlays (cl-find-if #'... overlays))) (+org--toggle-inline-images-in-subtree beg end) (if (or image-overlays latex-overlays) (org-clear-latex-preview beg end) (org--latex-preview-region beg end)))) ((eq type 'clock) (org-clock-update-time-maybe)) ((eq type 'footnote-reference) (org-footnote-goto-definition (org-element-property :label context))) ((eq type 'footnote-definition) (org-footnote-goto-previous-reference (org-element-property :label context))) ((memq type '(timestamp planning)) (org-follow-timestamp-link)) ((memq type '(table-row table)) (if (org-at-TBLFM-p) (org-table-calc-current-TBLFM) (condition-case nil (progn (save-excursion (goto-char ...) (org-call-with-arg ... ...))) (error nil)))) ((eq type 'table-cell) (org-table-blank-field) (org-table-recalculate arg) (if (and (string-empty-p (string-trim (org-table-get-field))) (and (boundp 'evil-local-mode) evil-local-mode)) (progn (evil-change-state 'insert)))) ((eq type 'babel-call) (org-babel-lob-execute-maybe)) ((eq type 'statistics-cookie) (save-excursion (org-update-statistics-cookies arg))) ((memq type '(inline-src-block src-block)) (org-babel-execute-src-block arg)) ((memq type '(latex-environment latex-fragment)) (org-latex-preview arg)) ((eq type 'link) (let* ((lineage (org-element-lineage context '... t)) (path (org-element-property :path lineage))) (if (or (equal (org-element-property :type lineage) "img") (and path (image-type-from-file-name path))) (+org--toggle-inline-images-in-subtree (org-element-property :begin lineage) (org-element-property :end lineage)) (org-open-at-point arg)))) ((org-element-property :checkbox (org-element-lineage context '(item) t)) (let ((match (and (org-at-item-checkbox-p) (match-string 1)))) (org-toggle-checkbox (if (equal match "[ ]") '(16))))) (t (if (or (org-in-regexp org-ts-regexp-both nil t) (org-in-regexp org-tsr-regexp-both nil t) (org-in-regexp org-link-any-re nil t)) (call-interactively #'org-open-at-point) (+org--toggle-inline-images-in-subtree (org-element-property :begin context) (org-element-property :end context))))))
#+END_EXAMPLE

Printing the value of the =client= variable from inside =edebug= on =jupyter-kernel-info= yields

#+BEGIN_EXAMPLE elisp
;; client  ;;pp-eval-last-sexp
#s(jupyter-org-client
   (#<finalizer>)
   jupyter--clients "idle" 1 #s(hash-table size 65 test equal rehash-size 1.5 rehash-threshold 0.8125 data
                                           ())
   nil #s(jupyter-channel-ioloop-comm
          (#s(hash-table size 1 test eql weakness value rehash-size 1.5 rehash-threshold 0.8125 data
                         (t #0)))
          #s(jupyter-zmq-channel-ioloop
             (#<finalizer>)
             #<process zmq> nil
             ((send
               ((channel jupyter-channel)
                msg-type msg msg-id)
               ((list 'sent
                      (oref channel type)
                      (jupyter-send channel msg-type msg msg-id))))
              (stop-channel
               (type)
               ((let
                    ((channel
                      (object-assoc type :type jupyter-channel-ioloop-channels)))
                  (when
                      (and channel
                           (jupyter-channel-alive-p channel))
                    (jupyter-stop-channel channel))
                  (list 'stop-channel type))))
              (start-channel
               ((channel jupyter-channel)
                endpoint)
               ((when
                    (jupyter-channel-alive-p channel)
                  (jupyter-stop-channel channel))
                (oset channel endpoint endpoint)
                (let
                    ((identity
                      (jupyter-session-id jupyter-channel-ioloop-session)))
                  (jupyter-start-channel channel :identity identity))
                (list 'start-channel
                      (oref channel type)))))
             ((setq jupyter-channel-ioloop-session
                    (jupyter-session :id "38bcac68-f74f-4bd2-b1e7-998df7c14c4f" :key "a1369b21-aa7ff7834dd1f3fa5f7108e7"))
              (require 'jupyter-channel-ioloop)
              (require 'jupyter-zmq-channel-ioloop)
              (push 'jupyter-zmq-channel-ioloop--recv-messages jupyter-ioloop-post-hook)
              (cl-loop for channel in
                       '(:shell :stdin :iopub)
                       unless
                       (object-assoc channel :type jupyter-channel-ioloop-channels)
                       do
                       (push
                        (jupyter-zmq-channel :session jupyter-channel-ioloop-session :type channel)
                        jupyter-channel-ioloop-channels)))
             ((mapc #'jupyter-stop-channel jupyter-channel-ioloop-channels)))
          #s(jupyter-hb-channel :hb #s(jupyter-session
                                       (:shell_port 60543 :iopub_port 52071 :stdin_port 46591 :control_port 37599 :hb_port 49243 :ip "127.0.0.1" :key "a1369b21-aa7ff7834dd1f3fa5f7108e7" :transport "tcp" :signature_scheme "hmac-sha256" :kernel_name "julia-1.5")
                                       "38bcac68-f74f-4bd2-b1e7-998df7c14c4f" "a1369b21-aa7ff7834dd1f3fa5f7108e7")
                                "tcp://127.0.0.1:49243" #<user-ptr ptr=0x6000002f88a0 finalizer=0x10e782ed0> 10 ignore t t)
          jupyter-zmq-channel-ioloop #s(jupyter-session
                                        (:shell_port 60543 :iopub_port 52071 :stdin_port 46591 :control_port 37599 :hb_port 49243 :ip "127.0.0.1" :key "a1369b21-aa7ff7834dd1f3fa5f7108e7" :transport "tcp" :signature_scheme "hmac-sha256" :kernel_name "julia-1.5")
                                        "38bcac68-f74f-4bd2-b1e7-998df7c14c4f" "a1369b21-aa7ff7834dd1f3fa5f7108e7")
          (:stdin #s(jupyter-proxy-channel "tcp://127.0.0.1:46591" t)
                  :shell #s(jupyter-proxy-channel "tcp://127.0.0.1:60543" t)
                  :iopub #s(jupyter-proxy-channel "tcp://127.0.0.1:52071" t)))
   #s(jupyter-session
      (:shell_port 60543 :iopub_port 52071 :stdin_port 46591 :control_port 37599 :hb_port 49243 :ip "127.0.0.1" :key "a1369b21-aa7ff7834dd1f3fa5f7108e7" :transport "tcp" :signature_scheme "hmac-sha256" :kernel_name "julia-1.5")
      "38bcac68-f74f-4bd2-b1e7-998df7c14c4f" "a1369b21-aa7ff7834dd1f3fa5f7108e7")
   #s(hash-table size 65 test equal rehash-size 1.5 rehash-threshold 0.8125 data
                 ())
   #s(jupyter-kernel-process-manager
      (#<finalizer>)
      jupyter--kernel-managers #s(jupyter-command-kernel
                                  (#<finalizer>)
                                  ("julia-1.5" "/ssh:notebooks-vm.us-central1-f.quarere|docker:klt-notebooks-vm-cjme:/home/jovyan/.local/share/jupyter/kernels/julia-1.5" :argv
                                   ["/usr/bin/julia" "-i" "--startup-file=yes" "--color=yes" "--project=@." "/home/jovyan/.julia/packages/IJulia/tOM8L/src/kernel.jl" "{connection_file}"]
                                   :env nil :display_name "Julia 1.5.1" :language "julia" :interrupt_mode "signal" :metadata nil)
                                  #s(jupyter-session
                                     (:shell_port 60543 :iopub_port 52071 :stdin_port 46591 :control_port 37599 :hb_port 49243 :ip "127.0.0.1" :key "a1369b21-aa7ff7834dd1f3fa5f7108e7" :transport "tcp" :signature_scheme "hmac-sha256" :kernel_name "julia-1.5")
                                     "38bcac68-f74f-4bd2-b1e7-998df7c14c4f" "a1369b21-aa7ff7834dd1f3fa5f7108e7")
                                  #<process jupyter-kernel-julia-1.5>)
      #s(jupyter-zmq-channel :control #s(jupyter-session
                                         (:shell_port 60543 :iopub_port 52071 :stdin_port 46591 :control_port 37599 :hb_port 49243 :ip "127.0.0.1" :key "a1369b21-aa7ff7834dd1f3fa5f7108e7" :transport "tcp" :signature_scheme "hmac-sha256" :kernel_name "julia-1.5")
                                         "38bcac68-f74f-4bd2-b1e7-998df7c14c4f" "a1369b21-aa7ff7834dd1f3fa5f7108e7")
                             "tcp://127.0.0.1:37599" #<user-ptr ptr=0x6000002f8900 finalizer=0x10e782ed0>))
   #<buffer  *jupyter-kernel-client*> nil "null" nil nil nil)
#+END_EXAMPLE

When I check all of the existing kernel files, I find one whose kernel ID is different ( =14238987-f1b4-4049-982a-94012ddb7087= )from what is contained in the =client= variable ( =38bcac68-f74f-4bd2-b1e7-998df7c14c4f= ), but whose key and various ports are all correct.

#+BEGIN_EXAMPLE sh
/docker:klt-notebooks-vm-cjme:/home/jovyan/ #$ cat .local/share/jupyter/runtime/kernel-14238987-f1b4-4049-982a-94012ddb7087.json
{
  "shell_port": 60543,
  "iopub_port": 52071,
  "stdin_port": 46591,
  "control_port": 37599,
  "hb_port": 49243,
  "ip": "127.0.0.1",
  "key": "a1369b21-aa7ff7834dd1f3fa5f7108e7",
  "transport": "tcp",
  "signature_scheme": "hmac-sha256",
  "kernel_name": "julia-1.5"
}
#+END_EXAMPLE

This suggests that the root of the problem is that the kernel ID is not being captured accurately. There is no kernel with an ID equivalent to the one that appears in the =client= variable, so it is not clear to me where the value that appears in the client variable is coming from. I don't know if there is any condition in which jupyter changes the kernel ID and leaves all other parameters the same.

The issue appears to arise in =jupyter-make-client= (see [[https://github.com/nnicandro/emacs-jupyter/blob/a9ae0bcef52a62cf7df520756d994162a0570156/jupyter-kernel-manager.el#L141][L141 in jupyter-kernel-manager.el]]) when calling =make-instance class= for class =jupyter-org-client= which derives from =jupyter-repl-client= which derives from =jupyter-kernel-client=. The =session= is defined as an element of =jupyter-kernel-client= at [[https://github.com/nnicandro/emacs-jupyter/blob/403c70c83cb3754c83da0932b0efaf5e72bdca9a/jupyter-client.el#L215][L215 of jupyter-client.el]].
** Arxana

Now that we’ve gotten quite familiar with Org Mode things, it could be
good to revisit our work with Arxana and see what it might have to
offer here.  We know that Arxana is similar to [[https://github.com/opencog/atomspace][Atomspace]].  It would be
possible to create a (Clojure) backend that’s linked with [[https://github.com/juxt/crux][Crux]] or some
other graph database.

When we left off working on Arxana in 2017, we made a few notes.
Maybe it could be applied to something interesting for which we could
get a grant. Below, is some of the future work (circa FARM 2017).

The code is stored in [[https://repo.or.cz/w/arxana.git][https://repo.or.cz/w/arxana.git]] (with the latest
in the mob branch).  There’s some propaganda about the project at
[[http://arxana.net/][http://arxana.net/]].

*** 2021 report: Dusting off the Tome

I’m starting to revive Arxana for use in grant applications
(targeting [[*EPSRC: Mathematical Sciences Small Grants][EPSRC: Mathematical Sciences Small Grants]]).  As I go along I want to
better understand how the old versions of the code works, and better
understand the use cases for which something like Arxana could be
relevant.

**** Use case: Edit transcluded files                               :usecase:

- I have a bunch of org files managed by Org Roam
- I also have [[https://github.com/exp2exp/exp2exp.github.io/blob/master/src/manual/org-roam-manual.el][code for pulling them together into one big Org file]].
- I'd like to be able to edit the big Org file, and have the changes flow back to all of the underlying Org Roam files without having to think about saving or copying and pasting.

This particular example should in principle be doable with [[https://github.com/nobiot/org-transclusion][org-transclusion]], but looking into it a bit more, that’s not actually how the existing implementation works:

#+begin_quote
Transcluded contents are read-only. To edit them, use
=org-transclusion-open-edit-src-buffer-at-point= command. By default, it
is bound to C-c n e in org-transclusion-mode-map (modify the
keybinding as you prefer).
#+end_quote

On the other hand, a limited /working/ version of the workflow is
afforded by =M-x occur= followed by =e=, which allows editing the /lines/
where a given word occurs.  But this is much more limited than
a full-transclusion based workflow.


**** Probe: Get Arxana 2005 running                                   :probe:

I got Arxana 2005 running again, with minor updates to the code.
This recipe now works to get a first look at what’s going on.

1. Find =sbdm4cbpp.tex=.
2. Evaluate the code block in the section called “Preface”.
3. Enter =C-; d a section RET= to browse a list of major sections (change to the =*Generic List*= buffer if it doesn’t pop up automatically).  Hit =RET= in the  =*Generic List*= buffer to view listed contents.
4. Follow colorful links in the =Main Article Display= or =Scholia Display= buffers with =C-; f=.
5. Type =C-; C-h= to list other keyboard commands.

**So far, nothing we haven’t seen in Org Mode.*

In particular, even though the system /imports/ bits of =sbdm4cbpp.tex= as
the demo, it’s not clear that it actually allows the user edit these
pieces and save them back into the source (as described above).
Exploring some of the other commands

***** Adding a scholium and editing an article

It was possible to create a scholium about a given article with =C-; m
p=, i.e,. =make-scholium-about-part-of-current-article=, then edit the
article and commit changes with =C-; c=, i.e., =commit-edits=, and notice
the text moving around appropriately.  So, that’s a little bit new.

*** Old agenda for developing Arxana (circa our FARM paper)
**** Immediate upcoming: Before September 3-9 ICFP conference, focusing on our demo
***** Clean up namespaces of functions                          :maintenance:
E.g. write =honey:add-nema= instead of =add-nema=.
***** find the generic interface layer and put it in its own file :maintenance:
Sketched in honey-spec.org, but let's make sure it's consistent.
***** Assemble/disassemble a buffer from/to distributed storage  :demoscheme:

While not needed for our FARM demo, this could be useful for the
Scheme demo because it makes a relatively convincing case.  And
furthermore this would be good as user facility and for us as we
continue working on the project.

- E.g., related work of arxana-merge merge automatically.

***** Write a basic IATCD evaluator to load Listing 1 stuff        :demofarm:

/What is a better name for IATCD?/ ;-)

This is just at the level of moving data around - specifically turning
s-expressions into triples.

***** Use scholium-based programs to run Listing 2                 :demofarm:

This is a minimal *working* implementation of what we talk about in the paper.

***** How do inferential connections work (I/R, I/E, etc.)?        :demofarm:

E.g. fig. 7 of Lytinen.

This is just for exposition, when talking about prior art it would be nice to know how they did it.

- links between basic CD's
 - e.g., joe communicated the IP address to ray, by talking, so that ray could get on the server

**** Next steps: Paper for IJCAI 2018
***** IJCAI 2018: Write up applications to mathematics             :platform:
Possibly for IJCAI/ECAI.
Papers due *January 2018*? http://www.chessprogress.com/IJCAI-2018/calls/
To take place *July 13-19, 2018*
***** Demo the system walking through the steps of a proof like GCP or MPM.
***** Demo with APM prelim problems
This might be a "future work" section for this paper.
***** Demo with APM-Xi content
Show interface with types.
- E.g. APM-Xi style formulations of category theory definitons could be salient to work with.

**** Next steps: Paper for ICFP 2018
***** ICFP 2018: Logic programming like Reasoned Schemer but for hypergraphs :platform:
Possibly submit to [[http://conf.researchr.org/home/icfp-2018][ICFP]]. Papers due *Fri 16 Mar 2018*.
Event to take place in St. Louis, Missouri, United States, to take place *late September*.
***** Fuzzy search to retrieve loose matches and analogies
***** Write a simple user language and an interface that generates triplets/quintuplets
***** Can the system come up with answers to new, basic, questions?

- Inspired by Nuamah et al's geography examples
- Simple comparisons, like, how does this concept relate to that concept?  We informally talk about this as ``analogy'' between concepts.  But...

***** Foldable views (like in Org mode) so that people can browse proofs

- This may come after the May submission
- Folding and unfolding the definitions of terms in something like an APM context is relevant example.  Just `unpacking' terms.
- Note that there is some relevant prior work in the "Wikum" paper of Amy Zhang et al

**** Other next steps: from the Future Work section of our FARM paper

This could potentially be used as the basis of an ERC fellowship
proposal.  The "2018" version of the call was released August 3 2017,
and is due October 17 2017.  Presumably the "2019" version of the call
will be run on a similar timeline.  A long PDF describing the current
call is here: [[http://ec.europa.eu/research/participants/data/ref/h2020/other/guides_for_applicants/h2020-guide18-erc-stg-cog_en.pdf][=h2020-guide18-erc-stg-cog_en.pdf=]]

***** Formal proof
****** Demo the system walking through the steps of a proof like GCP or MPM.
If we keep at it, might have this ready by January, in time for an
IJCAI paper.
****** Refine both representations and reasoning aspects.
****** Integrate external computer algebra / proof checking systems.
***** Embodiment and cognitive science
****** Build on CD theory to reason about embodied intuitions in geometric problems, integrate with Lakoff and Núñez's conceptual metaphors \cite{kaliszyk2014developing-misc}.
***** Linguistics and NLP
****** Integrate parsers to generate IATC+CD automatically.
****** Use these models to seed statistical machine learning, e.g., expanding on the work of Kaliszyk et al who ascertained the frequency of various schematic usages like ``let \(X\) be a \(Y\)'' in a specific corpus of proofs.
***** Machine learning
****** Integrate with knowledge bases of mathematical terms and frequency data (as above).
****** Model Stack Exchange dialogues, in parallel with the work done  on Reddit discussions \cite{zhang2017characterizing}.
****** Build a system with multiple agents that ``converse with each other to sharpen their wits'' \cite{heretical-theory}.


* Which model construction process works as a whole?
:PROPERTIES:
  :tag: HL
  :END:

We are working in an applied way to build models, starting with data
and using existing tools and methods, but without any strong guarantee
that we will find the most effective methods right away. So, with
these experiments we are investigating the process of “model
construction” generally understood. One example is building
computational structures from natural language and technical texts.

** Subgoals :noexport:
:PROPERTIES:
:ID:       0e2b1ab1-9e3a-4e6c-b2a7-e423cb41a030
:END:

- [[*Information extraction from SO Q&A items][Information extraction from SO Q&A items]]
** Information extraction from SO Q&A items
:PROPERTIES:
  :tag: CDN TO
  :END:
#+CATEGORY: ML

We are attempting to extract triples from textual Q&A by using a
Neural Machine Translation approach.

*** BACK Refinining OpenIE approach

**** Idea is we need a basic data model from which we can build things

- User assistants
- If we have triples we can do interesting subtasks
- What’s a nice little task to solve in workflows w/ text or w/ code?

*** GPT/BERT

- Are they relevant for this? Or are they just good for prettifying
   text to do fun tricks?

**** Ontology papers in Stanford

Pinpointing what can and can’t be done. E.g., phrase structure. Linear
probes into the model, correlate weight structure of attention nodes
and classical phrase extractors.  This uses Stanford’s phrase
structure parser (which is based on correct phrase structure
extraction).

GPT & BERT have the /beginning/ of capturing. Classical embedding
results hold (King is to Queen...).  Pretrained model gets maybe 85%
of the classical task.

So, if we get a downstream task (relatively shallow linguistic task
X), then you have an expectation that with 10K examples to fine-tune,
you’ll get a decent outcome.

**** Deyan’s prior notebooks

Had borderline reasonable results, extracting triples that were
linguistically plausible SVO triples.  But these weren’t business
ready.  It would get confused about extracting only a fragment
(‘elephant’, not ‘pink elephant’).  Still, this gives validation that
concept extraction works.

**** Play with rewrite rules?
- Michael Kohlhase’s thesis deals with this (using unification)
- In language land, rewrites are just translations
- In ML, it’s just a model translating things

In a serious program language setting, Python 3.7, we’ll use the abstract
syntax tree of Python, doing formal bits

**** Categorise: Q or A?
- A/B test explanations?
- “Grammarly” for SO (but this is 2-3 times harder than improving documentation)
**** Extracting metadata
***** Given text as question answer, provide as much metadata as possible
***** Instead of triples, care about words that may not even be in there
***** Google Photos
They’ve used individual classifiers for any label of interest “church”, “cheesecake”.
They have many NN classifiers, one for each photo.

This shines b/c you can annotate people with names.  If I search for
“me plus my parents” I get exactly what I’d want.

This would be a bit intense if you have 1 million data for 2m gigabytes of address space.
**** Map: Q’s to A’s and vice versa
**** Identify duplicate answers
If someone answers, people don’t ask.
**** Identify relevant answers
***** CL: match “I’m buying what someone is selling”
***** Iterating or recursively doing this as a tree

- =(A (B (C D E F)))=

(This is pretty easy to evaluate.)

E.g. with Wikipedia internal links: do they reference as related or...

On SO there’s a second aspect: “I’m trying to achieve X but I’m
failing in this way.”  The answer is a rewrite.  Not a dependency but
it’s about mastery.

“Recommending comprehensive solutions for programming tasks by mining
crowd knowledge.”

Link text + surrounding context: does the target page link back?
And if it does link back they are of mutual importance.

Context will tell whether it’s a general or specific concept.

JC: Q/A can also be seen as a link.

**** Route questions based on expertise
This is something that people have looked at.
**** Why have a man page if you could turn SO into man pages that interact?
In general docs are trash, so you google and use SO for tasks.
Pandas docs are almost intentionally obfuscated, the examples are useless.

Competing with Google-for-StackOverflow isn’t a great plan

But could I improve the documentation itself?

***** Autogenerate better documentation for python
- Python is ubiquitous and there are a lot of SO
- There could also be demand
- Ontology could turn into TOC for the guide
**** Validating GPT as usable or not usable in...?
There’s a terminal that uses GPT. You could describe your CSS
and it changed an English description into a webpage template.
**** Given a schema it can generate a query.
There are text summarisation quips (e.g., generate abstracts).

**** If you extracted information this way we could use STAN to validate a hypothesis
E.g. get estimates about sizes of groups on SO.
**** Pie-in-the-sky
You could use nonparametric Bayesian models to ‘tame’ a neural network
and make it interpretable.  You can put it into an end-to-end
differentiable system, alternate generalisable with model structure.

/Tangled Program Graphs/
**** “Hate speech”
"How do I solve this sort in Python"
If I reply enough with Haskell, you can see I’m galling him... this is so much easier in Haskell.
You can go w/ stable differences when these 2 user are interacting.

This is a high-quality answer but in the context of all the answers
and questions, you find it’s actually hate speech.
***** Friendworld
It’s about frogs that are friendly. This is a Pepe the frog
meme. They’d post melancholy or fun frog...

With interspersed nazi shit.

Audioplayers can be completely destroyed by playing a certain record.

If you’re looking for honest learning exachanges they are more mundane.

E.g., *account for poor wording*.

***** BUT... Humans are good at understanding this but computers aren’t.

People were pointing out the subtle stuff, the problem was that there
wasn’t enough investment to do anything about it.

In Germany, Twitter filters holocaust denial; even the stuff they
(could) detect they don’t remove.  In the US, if you report it,
they’ll deny it.  (It’s a ‘prior restraint’ thing... it’s complicated
if you’re responding to someone’s complaint.)

Look at two Nazi related words and see if they form a hashtag.
=#jewspiracy= etc.

Filters are however very difficult.

- An automated white-knight that did the responding for you
- But they want you to engage...

You could do tricks, people started using =#proudboys= for something
else.

***** Example: how does responding to hate speech influence things?

Study tracking activity and challenges as to whether people continue
posting hate speech.

***** “Consider writing this in a more assertive way”
I wonder if possibly...

Guess the degree of someone by reading their email
**** Automatically generating docs from type signature
Maybe going for a language with static types could be a way to combine
free association in the structured data.

This is more robust than "write language and get code out."

“Write code with a bug, get SO Q&A back” (Crokage?)

Starting with working code.
How would you generate failing code.
How would you generate failing unit tests?
(E.g., “fuzzers” that generate near arbitrary run-ti)
Put in integers, get output. Generate wrong unit tests.


*** Overall commments

These are translation or compression style problems.

Code generation demos are pretty suspicious: GPT3 doesn’t make
off-by-one errors, it uses completely different function syntax.

Like the motivation behind it. Z was recently criticising
auto-generation of query program. The amount of time it takes to debug
the query.

“Count all the listings” but rather queried the database’s AirBnB
table. What if there are multiple tables w/ similar names?

If you put leashes on these things, using solid methods. 

*** We didn’t get one simple

*** Next steps                                                     :noexport:
:PROPERTIES:
:ID:       2ee512d9-60cf-443c-aa3d-ef8eb42789e9
:END:

- [[*Knowledge graph][Knowledge graph]]
- [[*Advances in knowledge mining from technical documents][Advances in knowledge mining from technical documents]]
** Knowledge graph
:PROPERTIES:
  :tag: LRD TO
  :END:
#+CATEGORY: KRR

Once we have a model of knowledge from Q&A items, e.g., in the form of
triples. we will want to be able to do something with this material.
One way in which it may be useful is in combination with an existing
knowledge graph.  For example, we can look at material from Concept
Net.  We may also have to make some of our own Concept Net-like
graphs.

*** Practical work

We can already take some practical steps here, along the lines of the
earlier papers:

- "Modelling the way mathematics is actually done", and
- "Towards mathematical AI via a model of the content and process of mathematical question and answer dialogues".

One possible strategy for further development would be to keep feeding
our [[*Forum][Forum]] information into this wiki, thinking about the wiki as the
basic grounds for a kind of informal knowledge graph.  This could
potentially relate to the [[*Arxana][Arxana]] project.

*** BACK Analyse a small sample of examples from s.o.


*** Next steps                                                     :noexport:

- [[*Teach arbitrary coding][Teach arbitrary coding]]
- [[*Recommender System][Recommender System]]
* Underlying foundation
:PROPERTIES:
  :tag: HL
  :END:

We believe that category-theoretic foundations will help us make
progress across different representations of code, process, model
building, and so on.

** Subgoals :noexport:
:PROPERTIES:
:ID:       6778531b-0a13-4596-89f8-df926202c3b0
:END:

- [[*Category theoretic glue][Category theoretic glue]]
- [[*Generating small graphs][Generating small graphs]]
** Category theoretic glue
:PROPERTIES:
  :tag: CDN
  :END:
#+CATEGORY: MATH

We want to develop enough theory that we can use it to frame our
experiments.  We are trying to do this in a computationally meaningful way.

*** Feature: Understand comma categories as a potential “backend”  :ray:zans:

*** Next steps                                                     :noexport:

- [[*How to Design Programs][How to Design Programs]]

** Probabilistic programming for scientific modelling
:PROPERTIES:
  :tag: HD
  :END:
#+CATEGORY: MATH

Probabilistic programming is useful within both scientific modelling,
and, potentially, as part of a program synthesis toolkit.

*** Feature: relationship between probabilistic programming and categories :zans:cameron:

*** Contributes to                                                 :noexport:

- [[*BUSINESS DEVELOPMENT][BUSINESS DEVELOPMENT]] (at least potentially, e.g., if our business is going to make models for people)
- [[*DATA COURSE][DATA COURSE]]
* POTENTIAL PRODUCTS
:PROPERTIES:
  :tag: HL
  :END:

Synthesis of some of our /projects/ could lead to marketable /products/.

** Contributes to                                                   :noexport:

- [[*BUSINESS DEVELOPMENT][BUSINESS DEVELOPMENT]]
** Agent model
:PROPERTIES:
  :tag: HD
  :END:

One of our central intentions is to instantiate our work in an agent
model of Q&A and programming.  This is based on Alan Turing’s
suggestion that computers could talk with each other to sharpen their
wits.

*** Next steps                                                     :noexport:
:PROPERTIES:
:ID:       17297f1e-d7e0-46d3-8a26-a51500be92b7
:END:

- [[*An ABM of the computer programming domain][An ABM of the computer programming domain]]
- [[*POTENTIAL PRODUCTS][POTENTIAL PRODUCTS]]
** Recommender System
:PROPERTIES:
  :tag: LRD
  :END:
#+CATEGORY: ML

We could consume various analyses of Stack Exchange data to make
recommendations.

*** Possible implementation strategy: build on a version of GPT fine-tuned on SO Q&A tasks

Could we set up a simple version of *GPT* trained on Stack Overflow
data, just to get it working? Then think about how to get a learning
loop set up to improve the results...

**** Ideas

- Could this at least help a human navigate the questions on Stack Exchange?
- Rather than just answering the question, generate the answer and use
  that to guide search (by combining generation with document similarity)
- Use a distance to set up a margin of tolerance

**** Precedents

- [[https://stackroboflow.com/about/index.html][Stack Roboflow]] creates ersatz Q&A using =AWD_LTSM=.  Surely we can do better?
- In Google Books, they use crappy OCR which is good enough for search, but you wouldn't want to read the output.  For search, they use something like rewrite distance, finding something ‘within 5 errors’.

**** Analogue

In parsing, it's not just edit distance but has to involve the grammar

**** Case against going too deep:

- Code generation is hard

**** Case against worrying about that:

- Worry instead about applications like generating learning packets
 - E.g., learn everything there is to know about =git= from Stack Overflow in a nicely organised way.
 - E.g., compare the Schuam’s Outline series: could we reassemble open source clones of Schuam’s Outlines by retrieving contents from Math.Stack Exchange?

**** Application of the model: Display SO with similarity graph
E.g., use generated answers to help identify ‘similarity’.

**** Related work

- https://github.com/stared/tag-graph-map-of-stackexchange/wiki presents a nice-looking map of the relationship between tags.

*** Feature: Initial import of SO for training                          :tim:
*** Contributes to                                                 :noexport:

- [[*Advances in tutoring systems for programming][Advances in tutoring systems for programming]]
- [[*Agent model][Agent model]]
- [[*Teach arbitrary coding][Teach arbitrary coding]]

** Visual Interfaces
:PROPERTIES:
  :tag: LRD
  :END:

*** Graphical flow for programs

Can we model more general program flow in a similar fashion to Monocl?

*** Limitations

The idea of graphical programming languages is linked with the
[[https://en.wikipedia.org/wiki/Deutsch_limit][Deutsch limit]] (named for noted programmer [[https://en.wikipedia.org/wiki/L._Peter_Deutsch][L Peter Deutsch]], not
physicist [[https://en.wikipedia.org/wiki/David_Deutsch][David Deutsch FRS]], though perhaps he could come into play later):

#+begin_quote
/The problem with visual programming is that you can’t have more than 50 visual primitives on the screen at the same time./
#+end_quote

*** Automatically create visual interfaces

Here's an idea: assuming we have enough text mining pixie dust (on
corpora of linux man pages, and stack overflow questions/forum posts
about linux commands), it might be possible to do:

=user:~$ make-gui-for ls --output ls.py=

*** Feature: Build infra for generating and displaying graphs.

E.g., we can generate graphs based on code flow.

#+begin_src elisp
(defun triangle (n)
  (if (equal n 0) 0
    (+ n (triangle (- n 1)))))
#+end_src

This would then be related to the visual code walk through feature described below.

*** Feature: Visual code walk through

Ray is working on a visual code walk through.  This should be seen as
another interface to the same basic underlying information, sort of
like how Org Roam is the main interface to the data served by Org Roam
Server.

**** General evaluation strategy for these demos:

- /‘Would anyone want to use this?’/
- E.g., in the case of Emacs "learn X in Y" demo.
- If there is interest, work up to covering the HtDP book

**** Related work

- MAUDE framework. :: You describe your programming language using
  rewrite rules in K.  They define tools to auto-derive rules in [[http://www.kframework.org/index.php/Projects][K]].

- Program slicing :: ‘Galois connection on the traces’. This allows
  you to find where bugs appeared.  People tend to look in the most
  recent.  Imagine a call-graph of all the variables, so it gives you
  a minimum trace, showing where your bug can be found.

*** Next steps                                                     :noexport:
:PROPERTIES:
:ID:       8ed6b549-0761-4f06-b478-d47e5ff1036f
:END:

- [[*Paperspace DO NJ etc. Collaboratory][Paperspace DO NJ etc. Collaboratory]]

*** Contributes to                                                 :noexport:
- [[*POTENTIAL PRODUCTS][POTENTIAL PRODUCTS]]
** Data course
:PROPERTIES:
  :tag: LRD
  :END:

There's a new book available from the group affiliated with STAN.  It
doesn't go very far, but it has tons of examples.  They have data sets
about all sorts of stuff.  So the idea would be to take, e.g., the
notebook on linear regression, and go through...

*** Idea

Start with a method, then go through lots of examples.  Make this
consistent with the way we would teach HtDP.

"Here's a data set, here's a method that would make sense to apply."

*** A quandry

Note that hand-coding of a curriculum vs making a general framework
that anyone can contribute to (e.g., to make their own curricula) are
pretty different things.  We will sort out this ambiguity later.

*** Sources

There are tons of great data sets, but the issue would be digging into
the details of some of them.  The real issue is coordinating.  We want
to start with e.g., intro to linear regression, then hierarchical
linear regression, and working up to things like Lotka-Voltera model.

- Datopian

*** How to build up to this?

- E.g., setting up the pre-requisites of the platform
- Setting up a tutorial on model building in a certain domain, get 10 people in the specialised tutorial, how is it received
- This would start building up the group of people
 - Using someone else's platform would be different from using our own platform
 - Which of these is the focus? (*Good question but let's have one or two sprints beforehand to see where things are going.*)

*** Assumptions

- Keep platform open source, assume people would want to use

*** Comments

- Platform is quite a general word, but in a way we are trying to make something easier
- The platform is just an interface to a piece of technology we build.  The core is really on the backend.
- So the focus should be on the backend not on the javascript bits.
- Maybe leverage more existing technologies for the platform, where building it basically means installing it.
- Nextjournal: this looks good because they have UX designers to polish things
- Cloud-based Emacs: Would allow you to back your instantiation as if Emacs is your operating system, 500GB instance on Google Cloud

*** Status

- Cameron has code to set up a multicluster platform available off the shelf that we can start with
- Ray has been doing similar things for personal use, though if this helps write biology papers.
- What if our user interface was Emacs?
 - Different keybindings; developers like Emacs or Vi...
 - Org Bable exists & we can refer to this for now

*** Reference

- Michael Betancourt: Towards a principled bayesian workflow

*** Next steps                                                     :noexport:

- [[*POTENTIAL PRODUCTS][POTENTIAL PRODUCTS]]
** Paperspace DO NJ etc. Collaboratory
:PROPERTIES:
  :tag: LRD
  :END:

This would be a potential user-facing product in which we could deploy
various curricula, share various tools for interacting with
scientific/computational models, and build a “knowledge hub” of people
who could do scientific work.

*** Contributes to                                                 :noexport:

- [[*POTENTIAL PRODUCTS][POTENTIAL PRODUCTS]]
- [[*DATA COURSE][DATA COURSE]]
* BUSINESS DEVELOPMENT
:PROPERTIES:
  :tag: HL
  :END:

** Relationship to purpose

Understanding how the business activities relate to the purpose?  We
might do things that appear unrelated what we say at *Why not what* to
serve customer needs in the mean time.  However, if we do, we should
either come up with some reasoning about how this helps us address the
purpose, or revise our statement of purpose to reflect the current
reality.  This presumably isn’t hard to do, e.g., we could say “once
we have a successful business we will pour /x%/ into research,” but in
any case we should clarify this.

** Roughly B2C

- Launch some version of the Emacs Hyper Notebook as a cloud service. (Build it first and test it first.)
- *Visual Interfaces*: Develop a user interface on top of more advanced data analysis tools. (The focus is on the infrastructure that allows you to convert a graph into a neural network or whatever.)
- *Data course* (training format): Recruit people to take our course for a fee.
- *Paperspace DO NJ etc. Collaboratory* (Edtech SaaS): People would build their own courses/projects on our software and pay for licensing.
- *Teach arbitrary coding* (Edtech SaaS): People would use our tutoring system to improve their programming abilities.

** B2B

- *Agent model* (software as a service format): We can run our agent model to generate new code or other insights. People can pay for compute plus a premium for quality.
- *Probabilistic programming for scientific computing* (Consulting format): going around and creating customers by talking to businesses, saying “Using proababilistic programming — or other technologies — we can optimize this, this, this, and this, saving you this much money.”
 - Many companies hardly use any AI, let alone deep learning. If you can hustle and sell things, this can work.
 - However, we don’t want to sell AI snake oil, so if we are going to do consulting it should be around topics that we’re actually experts on. For example, plausibly, we could talk about modelling /documents/ and /workflows/.

** Different kinds of users

If we want to build a business, we should focus on who our target
users actually are, and what problems we can solve for them.
Typically we would build the business in a customer-centric way.  So,
for example, are the users/customers:

- Advanced STAN users, or,
- People who don't know how to do data analysis but who can make graphs.

Broad categories of users are surveyed in the *Downstream*.

** Related work

- Be wary of competing with things like Roam, though some level of competition is intrinsic in business.
- “Roam scratches my itches for document and graph aware note taking pretty well.”

** Next steps :noexport:

- [[*Bottom][Bottom]]
** Upstream

This is a place to keep track of the upstream tickets we’re interested
in.  Our ability to resolve some of these issues might suggest that we
have an ability to deliver things that are useful to other people and
would therefore be a step in the direction of [[*BUSINESS DEVELOPMENT][BUSINESS DEVELOPMENT]].

*** emacs-jupyter

- [[https://github.com/nnicandro/emacs-jupyter/issues/296][:id not set in jupyter-start-kernel #296]]
- [[https://github.com/nnicandro/emacs-jupyter/issues/191]['Kernel did not respond to kernel-info request' when using ssh session. #191]]

*** Firn

- [[https://github.com/theiceshelf/firn/issues/57][links inside :noexport subtrees shouldn't generate backlinks in their targets #57]]
** Grant development
#+CATEGORY: BUSINESS

One way to do [[*BUSINESS DEVELOPMENT][BUSINESS DEVELOPMENT]] could be through grant development
to help bootstrap initial outputs.

A sensible place to get some grant funding connected into the themes
around this work would be through Joe’s appointment as a Research
Fellow at Oxford Brookes University’s Ethical AI Institute.

One of the constraints here is that that’s a 2-year appointment, and
some grants (but not all) require the PI to be a permanent employee.
But this constraint is something we could relatively easily work with.

*** EPSRC: Mathematical Sciences Small Grants

This is for projects of up to £80K.

https://www.ukri.org/opportunity/epsrc-mathematical-sciences-small-grants-scheme/

**** "Inducing a mathematics encyclopedia out of Arxiv"
We talked about following on from the Open Information Extraction
theme possibly in connection with the new preprint “Language Models
are Open Knowledge Graphs” (and the associated video [[https://www.youtube.com/watch?v=NAJOZTNkhlI][review]]).  This is
close to Deyan’s expertise and would build on his previous work.

#+begin_quote
**Deyan*: "Accurate entity linking is desired." (video min 43) -- hey,
they honestly admit this in the paper! This was my stopping point with
the jupyter notebook as well i believe, I would like to get the neural
model to "ace" that part of the job before moving further. [...]  ok,
so after watching the entire review, I can conclude if I arrive at a
reliably successful method following my initial notebooks, and apply
them to arXiv and/or SE there's certainly a paper in there. Their
suggestions from the paper aren't actually that useful sadly, just
confirming some basic practical tricks (using last attention layer,
constraining with beam search and using language priors for the triple
content) that were already known
#+end_quote
**** "Mining informal proof structure"
This would be a somewhat similar line of work but more along the lines
of what we did in the FARM paper.  We discussed the advant

**** Maybe we could move in another more formal (category theoretic) direction
Maybe we could build out some of the ideas that Zans and Ray have been
discussing. Potentially this could also involve
*** DONE [#A] Outline(s) for small grants                               :joe:
**** DONE [#A] Revise Academic Beneficiaries part of draft              :joe:
* RESEARCH OUTPUTS
:PROPERTIES:
  :tag: HL
  :END:

We would like to publish some papers, though as Deyan points out we
should only do this when we have high-quality results:

#+begin_quote
Deyan: /Every paper that is published for the sake of an academic's publication record, rather than for its scientific merit, is potent fuel for science denialism. The short-term shortcuts for a personal career, when compounded, cause long-term harm to the scientific endeavor./
#+end_quote

So, what can we do without shortcuts?

** Next steps :noexport:

- [[*Bottom][Bottom]]
** Advances in tutoring systems for programming
:PROPERTIES:
  :tag: RR
  :END:

This would be a survey paper that would inform our efforts to *Teach arbitrary coding*.
Follow references, start with ‘AI and tutoring’.

1. (2014) "An adaptation algorithm for an intelligent natural language tutoring system"
2. (2008) "A novel approach for constructing conversational agents using sentence similarity measures"

*** Helps implement                                                :noexport:
- [[*Teach arbitrary coding][Teach arbitrary coding]]

*** Contributes to                                                 :noexport:
- [[*RESEARCH OUTPUTS][RESEARCH OUTPUTS]]
** Advances in knowledge mining from technical documents
:PROPERTIES:
  :tag: RR
  :END:
#+CATEGORY: RESEARCH

This would be a survey paper that would inform our efforts on
**Information extraction from SO Q&A items* and the *Knowledge graph*
approach.  Note that if we can find survey papers that others have
done, that’s pretty much just as useful, and saves us a bunch of time.

*** DONE Reading "Machine Knowledge" paper                            :deyan:
*** Contributes to                                                 :noexport:

- [[*RESEARCH OUTPUTS][RESEARCH OUTPUTS]]
** An ABM of the computer programming domain
:PROPERTIES:
  :tag: RO
  :END:

This would be a paper writing up our agent model work.

The paper could also correspond to a “whitepaper” that talks about how
we are able to “mine” computer programs automatically.  This would
contribute to a long-term business in automated programming (and
potentially other kinds of automation work).

*** Contributes to                                                 :noexport:

- [[*RESEARCH OUTPUTS][RESEARCH OUTPUTS]]
* Bottom
:PROPERTIES:
  :tag: HL
  :END:

By the time we get to this point, we will have established some
impressive research outputs, a potentially profitable business, and a
teaching/upskilling platform for technical and scientific topics.

#+ATTR_HTML: :width 700px
#+ATTR_LATEX: :width \textwidth
#+CAPTION: Network view
[[file:org-roam-server-3oct2020.png]]

** Contributes to :noexport:
:PROPERTIES:
:ID:       d8c152d1-0d86-4c66-9105-a83b926a0275
:END:
- [[*Downstream][Downstream]]
** Downstream
:PROPERTIES:
  :tag: AN
  :END:
#+CATEGORY: USERS

What do our potential users look like?

*** Possible future users                                          :noexport:
:PROPERTIES:
:ID:       34ddbcd3-10a2-4d08-90d9-a489b7542fae
:END:

- [[*Consulting clients][Consulting clients]]
- [[*Scientific software developers][Scientific software developers]]
- [[*Automated tutoring system users][Automated tutoring system users]]
- [[*Programmers][Programmers]]
** Consulting clients
:PROPERTIES:
  :tag: SH AN
  :END:

We discussed the idea of doing consulting for clients who are
interested in using scientific models.

/Play through again as a consulting client/
# - [[xid:0caba40b-2561-4143-b2b1-55f3ddc3201b][Play through again as a consulting client]]
** Scientific software developers
:PROPERTIES:
  :tag: SH AN
  :END:

We imagine some software developers consuming “tutorial” content we
produce, and improving their skills and abilities as a result.

/Play through again as a consulting client/
# - [[xid:0caba40b-2561-4143-b2b1-55f3ddc3201b][Play through again as a scientific software developer]]
** Automated tutoring system users
:PROPERTIES:
  :tag: SH AN
  :END:

We imagine some students using AI software we develop.  In some cases
they could be “students”.  In other cases, they could already be
professional developers.

# - [[xid:0caba40b-2561-4143-b2b1-55f3ddc3201b][Play through again as an automated tutoring system user]]
** Programmers
:PROPERTIES:
  :tag: SH AN
  :END:

We imagine any programmer having some use for our tools.  “B2D”
(Business to Developer) is an emerging category of enterprise where we
can do interesting things.

# - [[xid:0caba40b-2561-4143-b2b1-55f3ddc3201b][Play through again as a programmer]]
* Organisational infrastructure
:PROPERTIES:
  :tag: HL AN TO
  :END:
#+CATEGORY: ORG

This section is mildly-technical appendix.  It looks at our
organisational infrastructure itself, including simple things like the
technologies we use for communication, and more involved things like
“how we communicate” more broadly.  (This is a good candidate for
splitting off into its own separate wiki, if for no other reason than
that it takes up a lot of space in the generated PDF.)

** Schedule and activities

Presently we are meeting 20 minutes a day at 4PM UK time, 11AM
Eastern, on Discord for a “coffee chat”.

Previously we tried to maintain a schedule of longer meetings (UK
evenings):

- *Monday*: Seminar
- *Wednesday*: Workshop
- *Friday*: Studio

That seemed to be too many meetings.  Whatever we do about regularly
scheduled meetings, we might want to look at how to best pursue of
**topics of mutual interest* such as:

- *Readings* on rewriting rules and production systems, and higher-dimensional graph-like things
- *Business development* around open source, knowledge management, etc.
- *Reviewing* the value add of Wiki ways of thinking and working, which we have a pretty broad range of experience with
- *R&D* around ‘lenses’ in ACT: structure for bi-directional transformations, to enable changes in a projection

So far, this Roadmap has gathered information on some of the topics
that have been discussed, but not all of the things that we could see
ourselves working on together.

As another activity we may want to get scheduled one or more sessions
focused on business stuff.

** Project orientation

Some of this will be different depending on whether we think of this
as a “business”, or as “a business of some specific nature”: primarily
centring on “who does this business do business with?”

- *Status* - where is the project right now?
 - Right now /this overall project/ is in a “project development” mode.
 - What are the (multiple) /success indicators/ or /proof points/ or /failure indicators/ for each of the projects? (E.g., going to the casino with $20, you might quit when you get below $10, you might leave when you get above $50.) E.g., need of customers for X, our credibility in X?
 - For the various sub-projects: one relevant thing is “how long is it before thing is likely to make money?” (AKA, “Cross-over.”) Or “what else is needed for this to make money?”
 - In particular: maybe take a couple months to see how things are going with a given sub-project? This gives evidence of what we can produce when we work together. We might then ask, who else would care to pay for this?
 - We have listed 4 active projects (https://miro.com/app/board/o9J_kmPNvaQ=/); maybe the blog is another one.
- *Roles and Responsibilities* - /who is handling the standard project roles, and what are they responsible for doing?/
 - Each individual sub-project is likely to have different requirements (e.g., some may need 2 people, some will need 1, etc.)
 - If there’s more than one person involved it becomes a parallel architecture
- *Goals* - /What will this project achieve?/
 - “If I do something valuable, the money will come later.”
 - Some of them we might be willing to take the risk of investing time and energy based on whether it looks directly useful to us.
 - Some, like a course, we may need the information about whether it’s likely to be taught.
 - Some could become a paper or the building block of a business: these can be small demo projects.
 - Alternatively, in a consulting mode, our role becomes understanding customer goals and helping rationalise work to fulfil them.
- *Resource Requirements* - /What (people, money, things) are needed to accomplish this project?  Where do they come from?/
 - We each individually need some money, but it’s not totally clear that the /company/ needs some money.
 - If we wanted to replace any one of us with an employee, then we’d have to have some funding source.
 - If the number of person-hours for the goal is quite high, then it’s unlikely for the goal to be achieved without funding.
 - E.g., what would we need to be able to do consulting?
- *People* - /Who are the people working on this project? Who can I ask for more information? How can I best get in touch with them?/
 - If we were to be doing consulting, then it becomes about serving specific customer needs.
- *Approach* - /What is the overall strategy for accomplishing this project?/
 - Whatever we choose (e.g., consulting vs product development) we should choose it based on some data and analysis.
 - Wherever we are now, the question is what’s needed to move ahead.

- *Workplan and Timeline* - What are the specific tasks needed to accomplish our goals? When might they happen? Who / what / when (in agile, we specify two).
 - Joe needs some job soon!
 - To do consulting we’d need to figure out customer need and credibility
 - To make progress on the AI directions we need some version of all the things up and running!
- *Communication Norms* - how have the project participants agreed to stay in touch? what, where and how often are regular meetings? Special ceremonies?
 - In 2 months we’ll have 2 more months of experience.  So we could then assess things.
 - In advance of that, we might start to understand the expections about how we would gather the data.
 - It should be pretty much fun, and if it’s not we’re kind of doing it wrong?
 - On an ongoing basis we should be able to check whether what we’re doing is effectively addressing the goals we have
- *Sponsor* - /the person who requires the output of the project and has allocated the resources for it (aka Customer in agile)/
 - So far we’re all sponsoring our own work on sweat equity
 - While also trying to be helpful & respectful to each other
 - EF was the sponsor at one time
 - Joe provided chips and dip but the event was strictly BYOB... as long as we’re here we’ll make the best out of.  Polka time!
- *Project Manager* - the person responsible for the drumbeat and tempo of the project, and for its administrative details, including good project management hygiene
- *Lead* - the person responsible to the Sponsor for making sure the project is accomplished and to the Team for making sure they are able to accomplish the project
 - Ray: project to build bridges between participants (e.g., systems bio, category theory, stats); this is related to the “transdisciplinary design” course
 - Joe: I’m less technically sophisticated
- *Team* - people working on the project
 - Everyone will have some constraints (like need $40K per year if it takes more than 20 hours per week)

*** Project Management Hygiene

- set SMART goals (Specific, Measurable, Achievable, Relevant and Time-based)
 - (See also: /Coaching and Mentoring/
.)
- understand tasks required to accomplish goals, then set realistic timeline
- create project plan in wiki
- regular, frequent check-ins to iterate plan (goal, priorities, etc.) if necessary
- after-action reviews at the end of project, including reflection/writeup of positives and deltas
- experienced, well-oiled teams requires less strict project management hygiene
- new, less-organized, or heterogenous teams require more attention to careful project management hygiene

*** STARTED [#A] Review the Exp2Exp backlog to decide which things are wontfix, what other categories need to be created, etc.
I can browse these locally with =C-c R e=.

*** BACK Make a list of actual topics of interest
If we were just doing “content production” we might think of a list of
chapters to write, or podcasts to produce. However, maybe those ways
of thinking and working don’t apply comfortably here.

*** BACK Make a project analysis of active projects
This could refer to Peter Kaminski’s "[[*Project orientation][Project orientation]]" described in [[*Organisational infrastructure][Organisational infrastructure]].

** Technology

Does https://github.com/orgs/exp2exp/projects/1 conflict, replace, or
serve a different function compared with Org mode agenda items?

*** DONE Figure out Github project(s) vs Org todos               :joe:cameron:

We won’t use Github stuff right now.  That’s better for dealing with a
public-facing contribution workflow when we have actual open source
things running with contributors.

** Subgoals :noexport:
:PROPERTIES:
:ID:       17468abb-5c17-458e-a053-72e6356bbad5
:END:

- [[*OBS recordings][OBS recordings]]
- [[*Discord server][Discord server]]
- [[*Code sharing platform][Code sharing platform]]
- [[*Blog][Blog]]
- [[*Wiki][Wiki]]
- [[*Forum][Forum]]
** Discord server
:PROPERTIES:
  :tag: OTS AN
  :END:

We set up a Discord server that we’re using for our meetings.  This
invite link should not expire: https://discord.gg/pArjt4p

(We also have a Zulip server set up, but currently we’re using it
less.)
** OBS recordings
:PROPERTIES:
  :tag: OTS AN
  :END:
#+CATEGORY: OBS

We talked about creating asyncronous recordings (screencasts,
audio). We also talked about possibly putting the audio recordings
into a threaded voice mail forum, but that's a somewhat different
application.

** Code sharing platform
:PROPERTIES:
  :tag: OTS AN
  :END:
#+CATEGORY: OTS

For now we have a Github organisation (https://github.com/exp2exp), as
well as a separate repo that contains these Org Roam notes, among
other things.  This could potentially be improved or upgraded in
various ways.

*** Comments

- Nextjournal is interesting
- It's like a Jupyter notebook
- It's like Org Bable so you can run code in any language within the same environment
- If I need to add a bash cell to a Julia notebook, it adds a kernel as needed at the run time
- If I install a bunch of libraries, and save the current environment in a docker container, you can import it
- It doesn't yet have an easy way to make an app?

*** What if you had a browser based version of Org Bable?

- You could have your notebook, backed by the ability to use Emacs

*** Examples

- Setting up a data science experiment
- Wadler et al. course in Agda in NextJournal
- But you can't easily treat this as ‘Org Roam’ (no bi-directional things)

*** Next evolution

We need a basic code sharing platform to get to work.  The next
evolution might look like what we’ve been calling the “Emacs Hyper
Notebook”?  However, some contributors are not interested in using
Emacs for everything.  And we can’t assume that users would be
interested in it either!
** Wiki
:PROPERTIES:
  :tag: OTS AN
  :END:
#+CATEGORY: OTS

#+COMMENT: OLD CONTENT:
#+COMMENT: The public facing version of these notes is available on a simple
#+COMMENT: “brain dump” web interface, at https://notes.exploretoexploit.com/posts/.

The public facing version of these notes is available on a simple web
interface, created by firn: [[https://exp2exp.github.io/][https://exp2exp.github.io/]]. This mirrors
the contents of our Org Roam directory.  Editing is explained below.

We can also view the contents of Org Roam in a linear form as PDF
document... or view the currently active tasks using Org Agenda.  In
the future we may want to have several different “upstream” locations,
based on several different small-scale wikis, all feeding into this
one location.  That’s not hard to set up.  Contents can also be
browsed in a graphical form either with the built in =org-roam-graph=
functionality, or by installing Org Roam Server and running
=org-roam-server-mode=.

We can potentially improve on all of this further, bulding something
like Metacademy.  For now, we describe how to use this simple Org Roam
based wiki.

*** Access

Obtain the sources by cloning the repo at [[https://github.com/exp2exp/exp2exp.github.io][https://github.com/exp2exp/exp2exp.github.io]].

#+begin_src 
git clone git@github.com:exp2exp/exp2exp.github.io.git
#+end_src

(See below for an alternative.)

*** Mob branch on repo.or.cz

We’re mirroring the repo to an environment that allows anonymous
commits (without need for further permissioning).  If want to
contribute anonymously, info on that is here: [[https://bit.ly/2EQRHEF][https://bit.ly/2EQRHEF]]

You can review commits to the mob branch here: [[https://repo.or.cz/arxana.git/shortlog/refs/heads/mob][https://repo.or.cz/arxana.git/shortlog/refs/heads/mob]]

*** Setup

Install Org Roam if needed (=M-x package-install RET org-roam RET=).

Subsequently, add this to your Emacs configuration:

#+BEGIN_src elisp
(require 'org-roam)
(setq org-roam-directory (concat "/home/"
                          (getenv "USER")
                          "/exp2exp/"))
(setq org-roam-completion-system 'helm)
(define-key org-roam-mode-map (kbd "C-c n l") #'org-roam)
(define-key org-roam-mode-map (kbd "C-c n f") #'org-roam-find-file)
(define-key org-roam-mode-map (kbd "C-c n b") #'org-roam-switch-to-buffer)
(define-key org-roam-mode-map (kbd "C-c n g") #'org-roam-graph)
(define-key org-mode-map (kbd "C-c n i") #'org-roam-insert)
(org-roam-mode +1)
#+END_src
*** Bonus feature: org-roam-checkout

If you regularly use your own separate Org Roam setup, you can use
this simple context switcher to move between the two.  Keep track of
the various separate Org Roam installations with =org-roam-library=
and then switch between them interactively with =org-roam-checkout=.

#+begin_src elisp
(defvar org-roam-library `(,(concat "/home/" (getenv "USER") "/exp2exp/")
                           ,(concat "/home/" (getenv "USER") "/org-roam/")))

(defun org-roam-checkout ()
  (interactive)
  (let ((ctx org-roam-directory))
    (if (eq (length org-roam-library) 1)
        ;; Still go ahead and set the variable in this case!
        (progn (setq org-roam-directory (car org-roam-library))
               (message "You only have one choice for org-roam-directory defined."))
      (let ((lib (completing-read "Choose a volume: " org-roam-library)))
        (when lib
          (setq org-roam-directory lib))))
    ;; assuming the user changes context, let’s also prompt them
    ;; to choose a new file in that context
    (when (not (eq ctx org-roam-directory))
      (org-roam-find-file))))
#+end_src

*** Interaction

Use the =C-c n f= keyboard command to add new disconnected nodes to
the graph, or use =C-c n i= to create a page and insert a wiki-style
link, like =[[New Page]]=. Follow links with =C-c C-o=. Display the
graph structure with =C-c n g=.  It may be necessary to run =M-x
org-roam-db-build-cache= to get the graph to match reality.  Add and
commit new or modified files with git, along with =org-roam.db=, and
push them to the repo.

*** Tags

Some of the nodes have =#+roam_tags= set:

| *code* | *meaning*        |
|--------+------------------|
| HL     | High level       |
| CDN    | Can do now       |
| LRD    | Longer R&D cycle |
| HD     | Has dependencies |
| RR     | Research Review  |
| RO     | Research Output  |
| OTS    | Off the shelf    |
| SH     | Stakeholder      |
| AN     | Annex            |

Some of the files also have a =#+CATEGORY= set.

*** Pairing

For syncronized browsing and editing with [[https://github.com/tjim/lockstep][lockstep.el]]:

#+begin_src
ssh pair@178.79.174.58
PW: <ASK JOE FOR THE PASSWORD>
emacsclient -a '' -t
M-x lockstep
#+end_src

To open up a real-time collaboration (with multiple cursors), use
[[https://code.librehq.com/qhong/crdt.el][crdt.el]], first to serve the buffer:

#+begin_src 
M-x crdt-serve-buffer
#+end_src

And then, from your client, to connect:

#+begin_src 
M-x crdt-connect
#+end_src

*** Linearizing
:PROPERTIES:
:ID:       5bbb2c06-bab3-4e66-8e52-c53642234dec
:END:

To turn this map into something we can reliably use, let’s try to
linearize it.

To downsample from Org Roam (save as =~/bin/roam2org.sh= and make it
executable):

#+begin_src bash
#! /bin/bash

emacs --batch -l ~/bin/downsample-org-roam.el --eval "(combine-files)" "$@"
#+end_src

Here are the working parts (save as =~/bin/downsample-org-roam.el=):

#+begin_src elisp :tangle yes
(defun downsample ()
  "Process an Org Roam buffer for inclusion in a standard Org file.
Changes title to header, and increase indentation of existing headers.
Changes file links to internal links."
  (if (looking-at "^#\\+TITLE:")
      (replace-match "*"))
  (forward-line 1)
  (if (looking-at "^#\\+roam_tags:\\(.*\\)")
      (replace-match ":PROPERTIES:
  :tag:\\1
  :END:"))
  (while (re-search-forward "^\\*" nil t)
    (replace-match "**"))
  (goto-char (point-min))
  (while (re-search-forward "\\[\\[file:\\([^]]*\\)\\]\\[\\([^]]*\\)\\]\\]" nil t)
    (replace-match "[[*\\2][\\2]]"))
  (buffer-substring-no-properties (point-min) (point-max)))

(defun combine-org-roam-files (&rest args)
"Combine a list of files, specified as ARGs.
The files are to be found in `org-roam-directory'."
  (apply #'concat
         (mapcar (lambda (file)
                   (save-window-excursion
                     (find-file (concat org-roam-directory file))
                     (let ((contents (buffer-substring-no-properties (point-min)
                                                                     (point-max))))
                       (with-temp-buffer (insert contents)
                                         (goto-char (point-min))
                                         (downsample)))))
                 (or (car args) (nthcdr 5 command-line-args)))))
#+end_src

*** Backlog
:PROPERTIES:
:ID:       665a10d6-f9b7-421f-bc63-745f4a597916
:END:

Part of the idea with a backlog is to go from most-doable, starting
with work in progress, to least-doable and potentially vague.  Here,
then, is one approximate linearization that may or may not meet that
description!

Note, this is duplicated in the index file, probably for sanity we
should pick one and automate the derived version from there!

#+begin_src elisp :tangle yes
(defvar files-to-combine
'("20200810131435-hyperreal_enterprises.org"
"20200810132653-top.org"
"20200905124558-why_not_what.org"
 "20200909195629-teach_arbitrary_coding.org"
 "20200810135851-how_to_design_programs_with_if.org"
"20200905124405-construct_critique_improve_models_of_the_creative_process.org"
  "20200905125342-emacs_hyper_notebook.org"
  "emacs_jupyter_remote_debugging.org"
"20200905125023-which_model_construction_process_works_as_a_whole.org"
 "20200905131027-information_extraction_from_so_q_a_items.org"
"20200905131918-knowledge_graph.org"
"20200905124432-underlying_foundation.org"
 "20200905125713-category_theoretic_glue.org"
 "20200905131656-probabilistic_programming_for_scientific_modelling.org"
"20201003205523-potential_products.org"
 "20200905130423-agent_model.org"
 "20200817172825-recommender_system.org"
 "20200810135457-visual_interfaces.org"
 "20200814203551-data_course.org"
 "20200905132603-paperspace_do_nj_etc_collaboratory.org"
"20200814210243-business_development.org"
"20200905134325-research_outputs.org"
 "20200810135325-advances_in_tutoring_systems_for_programming.org"
 "20200810135403-advances_in_knowledge_mining_from_technical_documents.org"
 "20200905132334-an_abm_of_the_computer_programming_domain.org"
"20200906003704-bottom.org"
 "20201003164408-downstream.org"
 "20201003165500-consulting_clients.org"
 "20201003170312-open_source_developers.org"
 "20201003170333-tutoring_students.org"
 "20201003171011-programmers.org"
"20200810135126-organisational_infrastructure.org"
 "20200810135619-discord_server.org"
 "20200811185435-obs_recordings.org"
 "20200814193042-code_sharing_platform.org"
 "20200912223428-wiki.org"
 "20201003164100-forum.org"
 "20200814195259-blog.org"
"sfi/sfi.org"
 "sfi/gather_data_via_stack_exchange_apis.org"
 "sfi/argumentation_theoretic_analysis.org"
 "sfi/process_model_analysis.org"
 "sfi/ml_nlp_bootcamp.org"
 "sfi/initial_ml_baseline_e_g_match_q_a.org"
 "sfi/hierarchical_ml_for_content_extraction.org"
 "sfi/active_inference_bootcamp.org"
 "sfi/agent_modelling_and_sandbox_setup.org"
 "sfi/curate_koans_and_develop_solver.org"
 "sfi/study_with_crowdsourced_exercises.org"
 "sfi/study_with_agent_written_questions.org"
 "sfi/publication_ijcai.org"
)
"An ordered list of files to combine in our export.
This is where the order of presentation in the downstream org file
and derived PDF is defined.")
#+end_src

To combine the files, run:
#+begin_src elisp
(combine-org-roam-files files-to-combine)
#+end_src

To get the indicative nesting (shown by spaces above) to be replicated
at the org level, run the following at the top of the exported
compilation:

#+begin_src elisp :tangle yes
(defun indent-org-roam-export ()
  "Utility function to increase indention for selected trees."
  (org-map-entries (lambda ()
                     ;; don’t demote the top level items and their sub-items
                     (let ((tag (org-entry-get nil "tag")))
                       (if (and tag (string= (car (split-string tag)) "HL"))
                           (progn (org-end-of-subtree)
                                  (setq org-map-continue-from (point)))
                         (org-do-demote))))
                   nil 'file))
#+end_src

Lastly, to rebuild the PDF, all of this can be done with one swift
action.

#+begin_src elisp :tangle yes
(defun rebuild-org-roam-pdf ()
  "Build an org file and PDF compiling `files-to-combine'."
  (interactive)
  (save-excursion (find-file (concat org-roam-directory
                                     "/manual/combined.org"))
    (goto-char (point-min))
    (search-forward "# IMPORT")
    (let ((beg (point)))
      (delete-region (point) (point-max))
      (insert "\n" (combine-org-roam-files files-to-combine))
      (goto-char beg)
      (indent-org-roam-export)
      (org-latex-export-to-pdf))))
#+end_src

*** Publishing to the web

Publishing with Firn is simple:

#+begin_src 
firn build
#+end_src

Then commit and push.

*** Reviewing progress

Something like the following should be all that’s get a high-level
overview of progress on active tasks, sourcing information directly
from the Org Roam files.  Add the following to your emacs
initialisation script (e.g., =~/.emacs=), evaluate it, and then run
=C-c r= to load up the fun.  This may not be the perfect presentation
yet but it gives an idea.

#+begin_src elisp
(setq org-todo-keywords
      '((sequence "TODO" "STARTED" "BLOCKED" "BACKBURNER" "FROZEN"
                  "|" "DONE" "DEFERRED" "WONTFIX")))

(setq org-agenda-sorting-strategy '((todo todo-state-down category-down)))

(setq org-agenda-files '("~/exp2exp/"))

(defun org-scrum-board ()
  (interactive)
  (org-todo-list "TODO|STARTED|BLOCKED|BACKBURNER|FROZEN|DONE|DEFERRED|WONTFIX"))

(global-set-key (kbd "C-c r") 'org-scrum-board)
#+end_src

This view can then be further filtered by regexp (e.g., your name) by
pressing ~=~.

*** Notes about exporting things with Firn

In regular Org mode, it’s possible to [[https://superuser.com/questions/726201/how-can-i-apply-easy-formatting-to-org-mode-blocks][define your own blocks]] which
then export to a div with the given block name:

#+begin_monoblock
This is some text.
#+end_monoblock

With Firn/Orgize, it seems these blocks aren’t currently exported.
However, examples are exported?  Maybe not.

#+begin_example
This is some text.
#+end_example

*** DONE Package downsamping code separately                            :joe:
*** WONTFIX Update the repo instructions to reference this file         :joe:

** Forum
:PROPERTIES:
  :tag: OTS AN TO
  :END:
#+CATEGORY: OTS

We talked about using Wikum as a forum, because we liked the idea of a
workflow based on summarising discussions. There’s now a demo instance
set up that we can use, here:

http://wikum.org/visualization_flags?id=590&owner=holtzermann17

*** Could we incorporate the ideas directly in Org or Org Roam?

Perhaps we could incorporate some Wikum ideas right into the wiki
here.  The idea would be to treat the top paragraph on each page as a
summary, and then add discussion threads below.  We’d want some system
of tags that indicated whether the summary was validated or now.
(Note the the original WikiWikiWeb did not have separate talk pages!
I don’t know if they practiced robust summarisation, either.)

***************** REMARK                                                :joe:
This is an “inline task,” via =(require 'org-inlinetask)=.  There
doesn’t seem to be support for nested or threaded tasks, but maybe we
would have use for non-threaded forum discussions at the end of any
page in the Wiki.  Incidentally, for those curious, the formatting of
the \LaTeX\nbsp{}export is controlled by
=org-latex-format-inlinetask-function=.
***************** END

*** BACK What might our summarisation workflow look like?
Since we’re pretty actively updating our *Discord* and pretty happy
using it, maybe people who are working on Active Projects would be
willing to summarise on the wiki, say, weekly?  And contribute to a
monthly group blog post?
** Blog
:PROPERTIES:
  :tag: OTS AN
  :END:
#+CATEGORY: BLOG

For now we’re just maintaining a /blog roll/ rather than one shared blog

#+begin_quote
Zans: /If I implemented as I read things, it would be a pretty interesting blog. There could be a huge market of people interested in following this, this would give a pool of people who know who we are. This is a nice goal b/c it doesn't focus on the product... but it's a deliverable, made up of smaller deliverables, and a concrete benefit./
#+end_quote

*** Blog roll

Some of us are writing blogs.

- [[https://exp2exp.github.io/updates][Updates on Exp2Exp]] by Joe Corneli
- [[https://prodg.org/blog][Prodg.org]] by Deyan Ginev
- [[https://zaeph.net/posts/][Zaeph.net]] by Leo Vivier
- [[https://exp2exp.github.io/erg][Emacs Research Group]] meeting notes

*** Blog roll: Others

Here are some people with related interests.

- [[https://sachachua.com/blog/][Sacha Chua]] maintains a great Emacs blog and newsletter
- [[https://weakty.com/chronolog][Weakty.com]] by T Sloane, the primary developer of Firn
- [[https://lyderic.origenial.fr/][Lydéric's Digital Garden]] by Lydéric Dutillieu, Clojure programmer, Paris
- [[https://doubleloop.net/][Double Loop.net]] by Neil Mather

*** Related                                                        :noexport:
:PROPERTIES:
:ID:       307bdc02-be3b-464b-8424-323b3c66981a
:END:

- [[*Code sharing platform][Code sharing platform]]

